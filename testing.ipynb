{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemprop.v2 import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemprop.v2 import featurizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemprop.v2.models import modules, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.progress import ProgressBar, RichProgressBar, TQDMProgressBar\n",
    "import sys\n",
    "class MyProgressBar(TQDMProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = super().init_validation_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar\n",
    "\n",
    "    def init_predict_tqdm(self):\n",
    "        bar = super().init_predict_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar\n",
    "\n",
    "    def init_test_tqdm(self):\n",
    "        bar = super().init_test_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/gridsan/nmorgan/MW.csv\") as fid:\n",
    "    reader = csv.reader(fid)\n",
    "    next(reader)\n",
    "    smis, scores = zip(*[(smi, float(score)) for smi, score in reader])\n",
    "scores = np.array(scores).reshape(-1,1)\n",
    "all_data = [data.MoleculeDatapoint(smi, target) for smi, target in zip(smis, scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nmorgan/mambaforge/envs/chemprop_gpu/lib/python3.11/site-packages/torch/utils/data/dataset.py:348: UserWarning: Length of split at index 2 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
     ]
    }
   ],
   "source": [
    "generator = torch.Generator().manual_seed(12)\n",
    "[0.81,0.09,0.1]\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(all_data, [0.5,0.5,0], generator=generator)\n",
    "\n",
    "featurizer = featurizers.MoleculeFeaturizer()\n",
    "\n",
    "train_dset = data.MoleculeDataset(train_data, featurizer)\n",
    "val_dset = data.MoleculeDataset(val_data, featurizer)\n",
    "test_dset = data.MoleculeDataset(test_data, featurizer)\n",
    "\n",
    "train_loader = data.MolGraphDataLoader(train_dset, num_workers=4)\n",
    "val_loader = data.MolGraphDataLoader(val_dset, num_workers=4, shuffle=False)\n",
    "test_loader = data.MolGraphDataLoader(test_dset, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(np.array([d._targets for d in train_dset.data]))\n",
    "molenc = modules.molecule_block()\n",
    "mpnn = models.RegressionMPNN(mpn_block = molenc, \n",
    "                             n_tasks = 1, \n",
    "                             init_lr = 5e-3,\n",
    "                             max_lr = 1e-2,\n",
    "                             final_lr = 5e-3,\n",
    "                             scaler = scaler,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/gridsan/nmorgan/mambaforge/envs/chemprop_gpu/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/gridsan/nmorgan/mambaforge/envs/chemprop_gpu/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/gridsan/nmorgan/chemprop/nathan/lightning_logs/version_23666795/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e7d358e7-783a-3a6f-9e59-ddd2825dcbd8]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | mpn_block | BondMessageBlock | 264 K \n",
      "1 | ffn       | Sequential       | 90.6 K\n",
      "-----------------------------------------------\n",
      "354 K     Trainable params\n",
      "301       Non-trainable params\n",
      "355 K     Total params\n",
      "1.421     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/home/gridsan/nmorgan/mambaforge/envs/chemprop_gpu/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it, v_num=2.37e+7, train/loss=0.00632]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 1/1 [00:01<00:00,  1.46s/it, v_num=2.37e+7, train/loss=0.00632]\n",
      "303.92321610450745\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    trainer = pl.Trainer(\n",
    "        enable_progress_bar=True,\n",
    "        devices=1,\n",
    "        max_epochs=200,\n",
    "        callbacks=[MyProgressBar()],\n",
    "        accelerator=\"gpu\",\n",
    "        default_root_dir=\"/home/gridsan/nmorgan/chemprop/nathan/\"\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    trainer.fit(mpnn, train_loader)#, val_loader)\n",
    "    t1 = time.time()\n",
    "    print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/gridsan/nmorgan/mambaforge/envs/chemprop_gpu/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | mpn_block | BondMessageBlock | 264 K \n",
      "1 | ffn       | Sequential       | 90.6 K\n",
      "-----------------------------------------------\n",
      "354 K     Trainable params\n",
      "301       Non-trainable params\n",
      "355 K     Total params\n",
      "1.421     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/home/gridsan/nmorgan/mambaforge/envs/chemprop_gpu/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it, v_num=2.37e+7, train/loss=0.0137] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it, v_num=2.37e+7, train/loss=0.0137]\n",
      "355.8001642227173\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    trainer = pl.Trainer(\n",
    "        enable_progress_bar=True,\n",
    "        devices=1,\n",
    "        max_epochs=200,\n",
    "        callbacks=[MyProgressBar()],\n",
    "        #accelerator=\"gpu\",\n",
    "        default_root_dir=\"/home/gridsan/nmorgan/chemprop/nathan/\"\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    trainer.fit(mpnn, train_loader)#, val_loader)\n",
    "    t1 = time.time()\n",
    "    print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/gridsan/nmorgan/MW_smi.csv\") as fid:\n",
    "    reader = csv.reader(fid)\n",
    "    next(reader)\n",
    "    smis = [smi[0] for smi in reader]\n",
    "\n",
    "pred_smis = [data.MoleculeDatapoint(smi) for smi in smis]\n",
    "pred_dset = data.MoleculeDataset(pred_smis, featurizer)\n",
    "pred_loader = data.MolGraphDataLoader(pred_dset, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e7d358e7-783a-3a6f-9e59-ddd2825dcbd8]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(tensor([[131.2663]]), None),\n",
       " (tensor([[185.0551]]), None),\n",
       " (tensor([[424.9604]]), None),\n",
       " (tensor([[115.8282]]), None),\n",
       " (tensor([[64.1624]]), None),\n",
       " (tensor([[131.5914]]), None),\n",
       " (tensor([[64.1624]]), None),\n",
       " (tensor([[142.2961]]), None),\n",
       " (tensor([[158.8229]]), None),\n",
       " (tensor([[55.9662]]), None),\n",
       " (tensor([[384.8159]]), None),\n",
       " (tensor([[366.7430]]), None),\n",
       " (tensor([[125.0626]]), None),\n",
       " (tensor([[144.1308]]), None),\n",
       " (tensor([[112.6024]]), None),\n",
       " (tensor([[115.1009]]), None),\n",
       " (tensor([[182.9586]]), None),\n",
       " (tensor([[119.3817]]), None),\n",
       " (tensor([[32.4084]]), None),\n",
       " (tensor([[86.1871]]), None),\n",
       " (tensor([[169.4939]]), None),\n",
       " (tensor([[61.5952]]), None),\n",
       " (tensor([[112.1386]]), None),\n",
       " (tensor([[128.4797]]), None),\n",
       " (tensor([[273.5119]]), None),\n",
       " (tensor([[119.1244]]), None),\n",
       " (tensor([[171.8700]]), None),\n",
       " (tensor([[83.3038]]), None),\n",
       " (tensor([[236.8674]]), None),\n",
       " (tensor([[56.3373]]), None),\n",
       " (tensor([[198.3642]]), None),\n",
       " (tensor([[116.3813]]), None),\n",
       " (tensor([[156.2687]]), None),\n",
       " (tensor([[94.8378]]), None),\n",
       " (tensor([[86.4418]]), None),\n",
       " (tensor([[145.2919]]), None),\n",
       " (tensor([[133.6800]]), None),\n",
       " (tensor([[81.3941]]), None),\n",
       " (tensor([[141.9831]]), None),\n",
       " (tensor([[99.4731]]), None),\n",
       " (tensor([[114.2854]]), None),\n",
       " (tensor([[306.4916]]), None),\n",
       " (tensor([[128.3756]]), None),\n",
       " (tensor([[88.8943]]), None),\n",
       " (tensor([[124.9061]]), None),\n",
       " (tensor([[102.1831]]), None),\n",
       " (tensor([[73.7560]]), None),\n",
       " (tensor([[227.8199]]), None),\n",
       " (tensor([[116.0395]]), None),\n",
       " (tensor([[99.7201]]), None),\n",
       " (tensor([[112.1574]]), None),\n",
       " (tensor([[50.5826]]), None),\n",
       " (tensor([[114.3069]]), None),\n",
       " (tensor([[131.4287]]), None),\n",
       " (tensor([[101.3294]]), None),\n",
       " (tensor([[147.2109]]), None),\n",
       " (tensor([[290.8262]]), None),\n",
       " (tensor([[99.7700]]), None),\n",
       " (tensor([[255.5372]]), None),\n",
       " (tensor([[128.8556]]), None),\n",
       " (tensor([[135.9106]]), None),\n",
       " (tensor([[103.5241]]), None),\n",
       " (tensor([[58.6157]]), None),\n",
       " (tensor([[113.9060]]), None),\n",
       " (tensor([[127.7490]]), None),\n",
       " (tensor([[210.4720]]), None),\n",
       " (tensor([[125.1844]]), None),\n",
       " (tensor([[345.6550]]), None),\n",
       " (tensor([[130.3222]]), None),\n",
       " (tensor([[169.1278]]), None),\n",
       " (tensor([[115.7495]]), None),\n",
       " (tensor([[64.1624]]), None),\n",
       " (tensor([[147.0209]]), None),\n",
       " (tensor([[320.7328]]), None),\n",
       " (tensor([[376.1154]]), None),\n",
       " (tensor([[100.6816]]), None),\n",
       " (tensor([[130.1699]]), None),\n",
       " (tensor([[392.8792]]), None),\n",
       " (tensor([[196.9186]]), None),\n",
       " (tensor([[124.0325]]), None),\n",
       " (tensor([[72.9612]]), None),\n",
       " (tensor([[356.6207]]), None),\n",
       " (tensor([[109.4938]]), None),\n",
       " (tensor([[112.7702]]), None),\n",
       " (tensor([[124.4284]]), None),\n",
       " (tensor([[222.3922]]), None),\n",
       " (tensor([[140.3580]]), None),\n",
       " (tensor([[103.8987]]), None),\n",
       " (tensor([[116.1709]]), None),\n",
       " (tensor([[114.7144]]), None),\n",
       " (tensor([[141.9409]]), None),\n",
       " (tensor([[137.3195]]), None),\n",
       " (tensor([[140.7517]]), None),\n",
       " (tensor([[62.8614]]), None),\n",
       " (tensor([[130.1519]]), None),\n",
       " (tensor([[70.0901]]), None),\n",
       " (tensor([[89.3494]]), None),\n",
       " (tensor([[333.7357]]), None),\n",
       " (tensor([[117.8266]]), None),\n",
       " (tensor([[112.8334]]), None)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(mpnn, pred_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 199,\n",
       " 'global_step': 200,\n",
       " 'pytorch-lightning_version': '2.0.6',\n",
       " 'state_dict': OrderedDict([('task_weights', tensor([[1.]])),\n",
       "              ('mpn_block.cached_zero_vector',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('mpn_block.W_i.weight',\n",
       "               tensor([[-0.0474, -0.0600, -0.0035,  ...,  0.0483,  0.0066, -0.0457],\n",
       "                       [-0.0081,  0.0427, -0.0458,  ...,  0.0313, -0.0575,  0.0215],\n",
       "                       [ 0.0518, -0.0777,  0.0018,  ..., -0.0209, -0.0603, -0.0278],\n",
       "                       ...,\n",
       "                       [ 0.0076,  0.0497, -0.0560,  ...,  0.0279,  0.0269,  0.0338],\n",
       "                       [-0.0087, -0.0148,  0.0424,  ...,  0.0772, -0.0041, -0.0282],\n",
       "                       [-0.0400, -0.0809, -0.0638,  ..., -0.0579, -0.0783,  0.0210]])),\n",
       "              ('mpn_block.W_h.weight',\n",
       "               tensor([[-0.0222, -0.0287, -0.0819,  ..., -0.0013,  0.0395, -0.0406],\n",
       "                       [-0.0214,  0.0131, -0.0416,  ...,  0.0168, -0.0181,  0.0084],\n",
       "                       [-0.0565,  0.0406, -0.0449,  ...,  0.0296, -0.0583, -0.0610],\n",
       "                       ...,\n",
       "                       [ 0.0485, -0.0228, -0.0162,  ..., -0.0053,  0.0293, -0.0325],\n",
       "                       [ 0.0183,  0.1050, -0.0166,  ..., -0.0349, -0.0440,  0.0016],\n",
       "                       [-0.0406, -0.0294, -0.0685,  ..., -0.0534, -0.0617, -0.0503]])),\n",
       "              ('mpn_block.W_o.weight',\n",
       "               tensor([[ 0.0155, -0.0143,  0.0248,  ...,  0.0240, -0.0025, -0.0033],\n",
       "                       [ 0.0120, -0.0445, -0.0367,  ..., -0.0569, -0.0640, -0.0186],\n",
       "                       [ 0.0463, -0.0071, -0.0382,  ..., -0.0408, -0.0014, -0.0571],\n",
       "                       ...,\n",
       "                       [ 0.0321, -0.0474,  0.0398,  ..., -0.0069, -0.0651, -0.0296],\n",
       "                       [ 0.0100,  0.0399,  0.0159,  ..., -0.0042,  0.0064, -0.0662],\n",
       "                       [ 0.0310, -0.0246,  0.0064,  ..., -0.0417, -0.0626,  0.0195]])),\n",
       "              ('mpn_block.W_o.bias',\n",
       "               tensor([-0.0326, -0.0238, -0.0552, -0.0383, -0.0431, -0.0007, -0.0725, -0.0719,\n",
       "                       -0.0149, -0.0133,  0.0043,  0.0214,  0.0099,  0.0289,  0.0424, -0.0388,\n",
       "                       -0.0697, -0.0484, -0.0265,  0.0605, -0.0340, -0.0020, -0.0676, -0.0309,\n",
       "                       -0.0276, -0.0861,  0.0126, -0.0367,  0.0718, -0.0158, -0.0081, -0.0372,\n",
       "                        0.0450, -0.0651, -0.0158, -0.0128,  0.0564, -0.0416, -0.0323, -0.0354,\n",
       "                       -0.0335, -0.0505,  0.0079, -0.0436,  0.0426,  0.0488, -0.0632, -0.0071,\n",
       "                       -0.0116, -0.0543, -0.0150, -0.0107,  0.0459, -0.0401, -0.0055,  0.0062,\n",
       "                        0.0212, -0.0181, -0.0236, -0.0645, -0.0565,  0.0260, -0.0569, -0.0051,\n",
       "                       -0.0209,  0.0455,  0.0624, -0.0269, -0.0134, -0.0775,  0.0613, -0.0035,\n",
       "                        0.0073, -0.0594, -0.0720, -0.0862, -0.0155, -0.0363, -0.0023, -0.0360,\n",
       "                       -0.0310, -0.0757,  0.0554, -0.0020, -0.0439, -0.0209, -0.0468, -0.0515,\n",
       "                       -0.0679, -0.0444,  0.0109, -0.0407, -0.0457,  0.0259, -0.0078,  0.0630,\n",
       "                       -0.0235,  0.0070, -0.0364,  0.0138,  0.0229,  0.0038, -0.0381, -0.0154,\n",
       "                        0.0640,  0.0226, -0.0616, -0.0464,  0.0171, -0.0095, -0.0422,  0.0024,\n",
       "                       -0.0683, -0.0135, -0.0341,  0.0674, -0.0381, -0.0513,  0.0438, -0.0027,\n",
       "                       -0.0553, -0.0131,  0.0311, -0.0069,  0.0489,  0.0236,  0.0143, -0.0493,\n",
       "                       -0.0015, -0.0005, -0.0299, -0.0465, -0.0486, -0.0343, -0.0133, -0.0584,\n",
       "                        0.0498,  0.0075,  0.0152,  0.0021, -0.0725,  0.0015, -0.0563,  0.0372,\n",
       "                        0.0164, -0.0476, -0.0410, -0.0217, -0.0123, -0.0004, -0.0519,  0.0281,\n",
       "                       -0.0298, -0.0061,  0.0128,  0.0216, -0.0742,  0.0908,  0.0234,  0.0141,\n",
       "                       -0.0472,  0.0186, -0.0616, -0.0755,  0.0101,  0.0182,  0.0648, -0.0501,\n",
       "                       -0.0304, -0.0341, -0.0119, -0.0624,  0.0223, -0.0442,  0.0377, -0.0114,\n",
       "                       -0.0194, -0.0162, -0.0220,  0.0467, -0.0320,  0.0357, -0.0210,  0.0055,\n",
       "                        0.0572, -0.0592, -0.0635,  0.0137,  0.0269,  0.0507, -0.0091, -0.0182,\n",
       "                       -0.0697, -0.0239,  0.0082, -0.0434, -0.0089, -0.0474, -0.0037, -0.0004,\n",
       "                       -0.0213, -0.0204, -0.0449,  0.0419, -0.0640, -0.0262, -0.0120, -0.0441,\n",
       "                       -0.0619, -0.0091, -0.0304, -0.0300, -0.0046, -0.0204, -0.0195, -0.0375,\n",
       "                       -0.0391, -0.0735,  0.0251,  0.0502, -0.0712,  0.0279, -0.0505, -0.0562,\n",
       "                       -0.0375, -0.0445,  0.0179, -0.0314,  0.0398, -0.0457,  0.0069,  0.0673,\n",
       "                       -0.0118,  0.0021, -0.0590,  0.0030, -0.0202, -0.0343, -0.0106,  0.0180,\n",
       "                       -0.0264,  0.0551,  0.0023, -0.0508, -0.0267, -0.0692, -0.0275, -0.0400,\n",
       "                        0.0304,  0.0033,  0.0160,  0.0194, -0.0618,  0.0425,  0.0583, -0.0441,\n",
       "                        0.0003, -0.0497,  0.0385, -0.0551,  0.0155, -0.0161,  0.0084, -0.0713,\n",
       "                        0.0126,  0.0377,  0.0341, -0.0527, -0.0534, -0.0046, -0.0459, -0.0513,\n",
       "                       -0.0272, -0.0637,  0.0137, -0.0408, -0.0391, -0.0382, -0.0051, -0.0180,\n",
       "                       -0.0962, -0.0331, -0.0392, -0.0507,  0.0654, -0.0287,  0.0022, -0.0172,\n",
       "                        0.0208,  0.0121,  0.0163,  0.0287,  0.0146, -0.0471, -0.0552, -0.0635,\n",
       "                       -0.0636, -0.0046, -0.0320,  0.0114])),\n",
       "              ('ffn.1.weight',\n",
       "               tensor([[-0.0296, -0.0472, -0.0073,  ..., -0.0588, -0.0842,  0.0235],\n",
       "                       [-0.0280, -0.0005,  0.0463,  ...,  0.0215, -0.0401, -0.0818],\n",
       "                       [ 0.0055, -0.0092, -0.0586,  ...,  0.0783,  0.0212,  0.0025],\n",
       "                       ...,\n",
       "                       [ 0.0191, -0.0202, -0.0613,  ..., -0.0867, -0.0112, -0.0612],\n",
       "                       [ 0.0210, -0.0147,  0.0116,  ...,  0.0114,  0.0417, -0.0420],\n",
       "                       [-0.0119, -0.0674, -0.0089,  ..., -0.0218,  0.0260, -0.0218]])),\n",
       "              ('ffn.1.bias',\n",
       "               tensor([ 0.0055,  0.0956,  0.0350,  0.0614,  0.0598, -0.0365, -0.0488,  0.0411,\n",
       "                       -0.0278, -0.0188,  0.0190, -0.0268, -0.0267,  0.0422,  0.0561,  0.0211,\n",
       "                       -0.0166, -0.0529, -0.0151, -0.0630, -0.0775, -0.0075,  0.0165,  0.0068,\n",
       "                        0.0185,  0.0279,  0.0085, -0.0440, -0.0592,  0.0177, -0.0592,  0.0568,\n",
       "                       -0.0062, -0.0113,  0.0342,  0.0705, -0.0028,  0.0127,  0.0709,  0.0047,\n",
       "                       -0.0817, -0.0065, -0.0045,  0.0779,  0.0173, -0.0453, -0.0093,  0.0293,\n",
       "                       -0.0225, -0.0948, -0.0727,  0.0277,  0.0124, -0.0271, -0.0016, -0.0609,\n",
       "                       -0.0391,  0.0123,  0.0038, -0.0601,  0.0052,  0.0360, -0.0655, -0.0047,\n",
       "                       -0.0278,  0.0467,  0.0569, -0.0663, -0.0032, -0.0207,  0.0362,  0.0282,\n",
       "                        0.0145, -0.0188, -0.0945, -0.0697, -0.0019, -0.0622, -0.0324,  0.0136,\n",
       "                       -0.0310, -0.0447,  0.1188,  0.0712,  0.0569,  0.0144,  0.0255, -0.0256,\n",
       "                       -0.0573, -0.0674, -0.0002,  0.0134, -0.0087,  0.0360, -0.0543, -0.0454,\n",
       "                        0.0725, -0.0052, -0.0430, -0.0436,  0.0338, -0.0260, -0.0144, -0.0275,\n",
       "                        0.0032, -0.0402,  0.0433, -0.0526, -0.0273, -0.0169,  0.0053, -0.0674,\n",
       "                       -0.0557, -0.0672,  0.0614,  0.0112,  0.0164,  0.0093, -0.0804, -0.0043,\n",
       "                        0.0384, -0.0463, -0.0103, -0.0340, -0.0581, -0.0263,  0.0903,  0.0355,\n",
       "                       -0.0409, -0.0493, -0.0477,  0.0804, -0.0282, -0.0737, -0.0378,  0.0714,\n",
       "                       -0.0460, -0.0679,  0.0164,  0.0076, -0.0111, -0.0542,  0.0964,  0.1226,\n",
       "                       -0.0833,  0.0543,  0.0296, -0.0373,  0.0654, -0.0478,  0.0044,  0.0131,\n",
       "                       -0.0593, -0.0133,  0.0739, -0.0309,  0.0014, -0.0583,  0.0510, -0.0842,\n",
       "                        0.0173, -0.0423, -0.0884,  0.0103, -0.0670,  0.0002, -0.0604, -0.0709,\n",
       "                       -0.0197,  0.0205,  0.0091, -0.0043, -0.0799, -0.0189,  0.0165, -0.0033,\n",
       "                        0.0031,  0.0102, -0.0237,  0.0292, -0.0155, -0.0347, -0.0147, -0.0352,\n",
       "                        0.0331,  0.0552,  0.0465,  0.0186, -0.0296,  0.0641, -0.0221, -0.0386,\n",
       "                       -0.0572,  0.0096, -0.0756, -0.0219, -0.0373,  0.0306, -0.0018, -0.0886,\n",
       "                       -0.0523, -0.0184,  0.0174, -0.0026,  0.0925,  0.0500,  0.0015, -0.0805,\n",
       "                       -0.0204,  0.0108,  0.0390, -0.0617, -0.0724, -0.0503,  0.0568, -0.0274,\n",
       "                       -0.0492, -0.0285, -0.0353, -0.0141,  0.0073, -0.0627,  0.0580,  0.0445,\n",
       "                       -0.0489,  0.0612, -0.0130,  0.0363, -0.0623,  0.0768, -0.0291,  0.0239,\n",
       "                       -0.0504,  0.0049,  0.0072,  0.0406,  0.0466,  0.0089,  0.0773,  0.0295,\n",
       "                       -0.0293, -0.0521,  0.0595, -0.0003, -0.0467,  0.0765,  0.0542,  0.0344,\n",
       "                       -0.0432, -0.0302,  0.0512, -0.0425,  0.0176, -0.0146,  0.0745, -0.0141,\n",
       "                       -0.0601,  0.0601, -0.0609, -0.0387,  0.0054,  0.0628,  0.0218,  0.0899,\n",
       "                       -0.0107,  0.0198, -0.0300,  0.0523, -0.0588, -0.0317, -0.0039, -0.0310,\n",
       "                       -0.0500, -0.0309, -0.0393,  0.0016,  0.0277, -0.0685, -0.0241, -0.0288,\n",
       "                        0.0423,  0.0178, -0.0629, -0.0591,  0.0142,  0.0233,  0.0739, -0.0462,\n",
       "                       -0.0448, -0.0468, -0.0646,  0.0004, -0.0274, -0.0324, -0.0229,  0.0057,\n",
       "                       -0.0340,  0.0468, -0.0203,  0.0204])),\n",
       "              ('ffn.4.weight',\n",
       "               tensor([[-6.8800e-04,  9.0009e-02, -2.3406e-02,  7.1226e-02,  4.8246e-02,\n",
       "                         1.4728e-02,  1.8521e-02,  8.5043e-02, -5.2964e-02,  1.6736e-03,\n",
       "                        -4.8280e-03, -8.8122e-03, -6.7101e-03,  5.5951e-02, -5.9590e-02,\n",
       "                         1.8705e-02,  2.0927e-03,  1.4554e-01,  1.6023e-02, -7.5317e-03,\n",
       "                         3.4237e-02,  7.2696e-03, -1.0204e-04,  2.4888e-02, -1.9790e-02,\n",
       "                         3.4143e-02, -2.0784e-02,  4.8568e-02, -3.1086e-02, -8.2151e-02,\n",
       "                         5.0125e-02,  6.1888e-03,  4.7288e-02,  1.2635e-02, -1.0269e-02,\n",
       "                         4.8051e-02,  4.4960e-02, -2.8166e-02, -6.2360e-02,  4.6915e-02,\n",
       "                        -1.4746e-02,  3.8013e-03, -1.4836e-03,  4.3503e-02,  1.0903e-01,\n",
       "                        -3.2583e-03,  9.9025e-02, -9.1413e-03,  8.3598e-04,  1.5034e-02,\n",
       "                        -9.6351e-03,  6.4440e-02,  3.3981e-02,  3.1380e-02, -3.9286e-03,\n",
       "                         4.3157e-04,  3.1444e-03, -3.8390e-03, -3.5889e-02, -1.2760e-02,\n",
       "                         6.2057e-02, -2.5186e-02,  8.4770e-03, -1.8046e-02,  1.5870e-02,\n",
       "                         9.1221e-02,  2.8017e-02,  1.9574e-02,  9.8993e-03, -2.8936e-02,\n",
       "                        -4.1583e-02,  3.1551e-02, -1.3894e-02,  1.1182e-02, -5.7558e-02,\n",
       "                        -5.0269e-04,  9.0699e-04, -1.4313e-03, -1.9917e-02, -5.7212e-02,\n",
       "                        -5.7155e-02, -3.7401e-02,  1.3326e-01,  8.0252e-02, -2.8245e-02,\n",
       "                         5.9260e-06, -3.0911e-02, -7.9777e-03, -1.9644e-02, -5.7031e-03,\n",
       "                        -3.0854e-02, -1.5872e-02,  8.5880e-04,  2.3843e-02,  1.6564e-02,\n",
       "                         2.2231e-02,  2.1853e-02,  7.1256e-03, -1.3838e-02, -1.0052e-02,\n",
       "                        -6.8985e-02,  8.4855e-04, -1.5840e-02, -2.8291e-02,  2.8138e-02,\n",
       "                        -2.2085e-02,  6.1240e-02, -1.2441e-02, -5.7688e-02, -5.4795e-02,\n",
       "                        -2.1264e-02, -7.8405e-04, -1.2615e-04,  1.2048e-02,  1.1399e-01,\n",
       "                        -5.5884e-02, -7.7161e-03, -2.8271e-02, -7.0177e-03, -1.6737e-02,\n",
       "                        -4.5531e-02,  1.0319e-02, -1.5514e-02, -1.8789e-02, -7.5550e-03,\n",
       "                        -1.1919e-02,  6.8948e-02, -9.2188e-02,  4.8681e-02, -2.6342e-02,\n",
       "                         3.3093e-02,  1.2306e-01,  4.6043e-02,  1.5694e-02, -4.7465e-02,\n",
       "                         3.2806e-02, -4.5566e-03, -1.0979e-02,  4.4018e-02, -1.1480e-02,\n",
       "                        -2.9290e-03,  3.8747e-02,  5.3432e-02,  1.0212e-01,  1.6202e-02,\n",
       "                        -1.6506e-02,  2.2455e-02, -3.1275e-02,  5.4277e-02,  2.5569e-01,\n",
       "                         2.4523e-02,  8.0544e-03,  2.1723e-02, -5.4844e-02, -3.3861e-02,\n",
       "                        -3.9616e-02, -6.4963e-02,  3.7440e-02, -8.8396e-02,  6.5449e-03,\n",
       "                         5.4661e-02, -4.4328e-02,  3.4714e-02, -5.5922e-02,  1.8996e-02,\n",
       "                        -3.9107e-02, -2.2340e-02,  4.2066e-02,  7.0014e-03, -1.8427e-02,\n",
       "                        -5.9243e-03, -1.6790e-02,  1.5770e-02,  3.2711e-03,  5.6063e-02,\n",
       "                        -1.3886e-02, -8.5145e-02, -3.0285e-02,  2.1134e-02,  1.2906e-01,\n",
       "                        -1.5205e-02, -1.6398e-02, -1.3969e-02, -5.5362e-02,  5.3232e-02,\n",
       "                        -6.4419e-02,  1.0604e-01, -1.7620e-02,  1.9088e-02,  1.9252e-01,\n",
       "                         1.5681e-02, -7.3114e-02,  2.1961e-02, -3.5358e-02,  1.8663e-02,\n",
       "                        -5.0654e-02,  2.4146e-03,  6.2491e-02, -8.9364e-03,  1.8095e-02,\n",
       "                        -4.1035e-03,  2.4679e-02, -1.6596e-02, -5.5018e-02, -6.6700e-02,\n",
       "                        -5.2393e-02, -4.6254e-03,  1.2534e-02,  1.0944e-02,  9.6540e-02,\n",
       "                        -3.9067e-02,  1.4274e-02, -5.9682e-03,  7.3378e-03,  5.9988e-02,\n",
       "                         5.1084e-02,  1.8149e-02,  6.4628e-03, -3.2893e-02, -5.2209e-02,\n",
       "                         4.8735e-02,  8.3890e-03,  4.6749e-02, -3.4307e-02,  6.1429e-03,\n",
       "                         2.1533e-02,  8.2900e-04, -4.2049e-02, -2.6020e-02,  4.8711e-02,\n",
       "                        -3.2224e-02,  6.6309e-03, -2.0723e-02, -4.6713e-02,  2.3319e-02,\n",
       "                        -3.5329e-02,  5.3639e-02,  6.2051e-02, -4.5813e-02,  1.1513e-01,\n",
       "                        -1.6296e-02,  1.9992e-02,  7.4210e-02,  1.9541e-02,  6.5574e-04,\n",
       "                         3.6735e-02,  4.0902e-02,  5.2114e-02,  1.3564e-02, -1.0442e-02,\n",
       "                         2.5371e-02,  7.4426e-02,  1.7946e-02,  5.1408e-02,  4.5982e-02,\n",
       "                        -8.7360e-02,  2.4886e-02,  1.5854e-01, -1.4740e-02, -2.0508e-02,\n",
       "                         2.2491e-02, -7.5139e-02, -8.2729e-03,  7.2050e-02, -3.2195e-02,\n",
       "                        -4.0696e-02, -4.4991e-02,  2.8127e-02,  2.6466e-02,  3.4158e-03,\n",
       "                        -3.7989e-02, -6.1244e-02, -1.7958e-02, -1.4467e-02,  6.1065e-03,\n",
       "                        -2.8353e-02,  4.8872e-02,  1.0634e-02,  2.1282e-02, -1.1117e-02,\n",
       "                        -3.1550e-02, -6.4036e-02, -4.1900e-02,  1.2039e-02,  2.4893e-02,\n",
       "                         3.8023e-02, -6.8855e-02,  7.3171e-03,  5.4619e-03, -2.4947e-02,\n",
       "                        -8.8474e-03, -7.9902e-03,  1.1052e-02,  1.3978e-02,  2.1043e-02,\n",
       "                         4.4222e-02, -7.4313e-03,  2.1539e-02, -1.9211e-02, -1.6869e-02]])),\n",
       "              ('ffn.4.bias', tensor([0.0108]))]),\n",
       " 'loops': {'fit_loop': {'state_dict': {},\n",
       "   'epoch_loop.state_dict': {'_batches_that_stepped': 200},\n",
       "   'epoch_loop.batch_progress': {'total': {'ready': 200,\n",
       "     'completed': 200,\n",
       "     'started': 200,\n",
       "     'processed': 200},\n",
       "    'current': {'ready': 1, 'completed': 1, 'started': 1, 'processed': 1},\n",
       "    'is_last_batch': True},\n",
       "   'epoch_loop.scheduler_progress': {'total': {'ready': 200, 'completed': 200},\n",
       "    'current': {'ready': 1, 'completed': 1}},\n",
       "   'epoch_loop.automatic_optimization.state_dict': {},\n",
       "   'epoch_loop.automatic_optimization.optim_progress': {'optimizer': {'step': {'total': {'ready': 200,\n",
       "       'completed': 200},\n",
       "      'current': {'ready': 1, 'completed': 1}},\n",
       "     'zero_grad': {'total': {'ready': 200, 'completed': 200, 'started': 200},\n",
       "      'current': {'ready': 1, 'completed': 1, 'started': 1}}}},\n",
       "   'epoch_loop.manual_optimization.state_dict': {},\n",
       "   'epoch_loop.manual_optimization.optim_step_progress': {'total': {'ready': 0,\n",
       "     'completed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0}},\n",
       "   'epoch_loop.val_loop.state_dict': {},\n",
       "   'epoch_loop.val_loop.batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False},\n",
       "   'epoch_progress': {'total': {'ready': 200,\n",
       "     'completed': 199,\n",
       "     'started': 200,\n",
       "     'processed': 200},\n",
       "    'current': {'ready': 200,\n",
       "     'completed': 199,\n",
       "     'started': 200,\n",
       "     'processed': 200}}},\n",
       "  'validate_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'test_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'predict_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}}}},\n",
       " 'callbacks': {\"ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\": {'monitor': None,\n",
       "   'best_model_score': None,\n",
       "   'best_model_path': '/home/gridsan/nmorgan/chemprop/nathan/lightning_logs/version_23652061/checkpoints/epoch=199-step=200.ckpt',\n",
       "   'current_score': None,\n",
       "   'dirpath': '/home/gridsan/nmorgan/chemprop/nathan/lightning_logs/version_23652061/checkpoints',\n",
       "   'best_k_models': {},\n",
       "   'kth_best_model_path': '',\n",
       "   'kth_value': tensor(inf),\n",
       "   'last_model_path': ''}},\n",
       " 'optimizer_states': [{'state': {2: {'step': tensor(200.),\n",
       "     'exp_avg': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       "     'exp_avg_sq': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]])},\n",
       "    3: {'step': tensor(200.),\n",
       "     'exp_avg': tensor([[-7.8159e-16,  1.4939e-14,  1.2569e-14,  ...,  2.9242e-15,\n",
       "              -5.0069e-15,  1.3951e-14],\n",
       "             [ 0.0000e+00, -1.2218e-03, -3.6408e-05,  ..., -2.5887e-14,\n",
       "              -1.0885e-03,  3.1172e-13],\n",
       "             [ 0.0000e+00, -4.7931e-05, -7.0930e-07,  ..., -8.9061e-14,\n",
       "              -1.6561e-05,  1.2735e-13],\n",
       "             ...,\n",
       "             [-3.2744e-16,  1.6695e-13,  7.6828e-14,  ...,  1.1321e-13,\n",
       "               9.6067e-14,  1.3122e-13],\n",
       "             [ 1.0086e-16, -1.0632e-04, -1.2003e-05,  ...,  2.5873e-14,\n",
       "              -3.9797e-04,  1.1927e-13],\n",
       "             [ 5.7591e-17,  5.6157e-13,  3.2927e-13,  ...,  4.5738e-13,\n",
       "               3.1326e-13,  4.8498e-13]]),\n",
       "     'exp_avg_sq': tensor([[8.1466e-14, 1.1035e-10, 6.1048e-12,  ..., 4.4673e-13, 5.7611e-11,\n",
       "              7.5605e-12],\n",
       "             [0.0000e+00, 5.6493e-05, 5.2279e-07,  ..., 1.0737e-09, 5.2613e-05,\n",
       "              3.9382e-09],\n",
       "             [0.0000e+00, 2.2616e-07, 9.3011e-09,  ..., 1.5353e-09, 8.7517e-08,\n",
       "              3.6972e-09],\n",
       "             ...,\n",
       "             [1.4298e-14, 3.4164e-09, 9.0913e-10,  ..., 9.5031e-10, 1.0572e-09,\n",
       "              9.3113e-10],\n",
       "             [1.3566e-15, 1.9819e-06, 1.0345e-07,  ..., 2.4962e-09, 7.2348e-06,\n",
       "              8.2496e-09],\n",
       "             [4.4231e-16, 1.8536e-08, 5.8827e-09,  ..., 1.8185e-08, 5.8992e-09,\n",
       "              2.0075e-08]])},\n",
       "    4: {'step': tensor(200.),\n",
       "     'exp_avg': tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -4.9100e-14,\n",
       "               7.5649e-15, -3.4612e-14],\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.8196e-14,\n",
       "               2.2983e-15,  5.2602e-14],\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.6978e-13,\n",
       "               1.7983e-13,  6.5671e-13],\n",
       "             ...,\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "               3.5155e-15,  4.3299e-16],\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.4326e-15,\n",
       "               4.0963e-15,  3.5550e-15],\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.1500e-12,\n",
       "               2.5312e-13,  1.2257e-12]]),\n",
       "     'exp_avg_sq': tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4585e-11, 5.3640e-10,\n",
       "              6.8214e-11],\n",
       "             [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5782e-10, 6.2620e-13,\n",
       "              3.0024e-10],\n",
       "             [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0658e-08, 1.6864e-09,\n",
       "              3.9080e-08],\n",
       "             ...,\n",
       "             [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.6481e-12,\n",
       "              2.5002e-14],\n",
       "             [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5069e-12, 1.5536e-12,\n",
       "              1.1238e-12],\n",
       "             [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1994e-07, 5.8459e-09,\n",
       "              1.4081e-07]])},\n",
       "    5: {'step': tensor(200.),\n",
       "     'exp_avg': tensor([ 1.2005e-12,  1.0014e-13,  1.1944e-12,  1.3797e-13,  5.4322e-12,\n",
       "              2.1591e-14,  6.5183e-16,  6.5689e-17, -2.4154e-05,  6.7493e-13,\n",
       "              9.0429e-04,  1.0556e-03,  6.8027e-04, -1.6595e-03, -1.2809e-03,\n",
       "             -1.7831e-04,  4.4202e-14,  1.7192e-12,  2.4615e-13, -4.6700e-06,\n",
       "              3.3136e-13,  3.6530e-12,  2.2330e-12,  9.0228e-04,  2.6317e-12,\n",
       "              5.0812e-12,  1.2291e-13,  3.1431e-14, -1.3118e-03,  2.6649e-13,\n",
       "              3.6658e-12,  8.1679e-13, -2.5710e-04,  1.9178e-12, -2.5208e-05,\n",
       "              7.9709e-07, -1.8639e-03,  7.9592e-15,  1.3928e-13, -8.0687e-06,\n",
       "              1.8557e-13,  1.2894e-13,  4.2772e-13,  2.5106e-15, -3.4437e-04,\n",
       "             -1.8035e-03,  4.3803e-13,  4.8746e-12,  2.8920e-15,  1.1394e-12,\n",
       "              9.1647e-04,  9.4911e-04, -2.4489e-04,  1.4512e-13,  1.9459e-11,\n",
       "             -5.6395e-07,  9.2404e-15, -1.0986e-04,  3.2012e-13,  5.6610e-13,\n",
       "              9.2822e-13,  1.1601e-03,  1.4146e-12,  1.1485e-13,  8.6181e-04,\n",
       "             -7.1312e-04, -5.9727e-04,  2.1595e-12,  2.3624e-14,  9.7598e-06,\n",
       "             -1.1114e-03,  2.1826e-12,  3.5306e-14,  7.6920e-13,  3.9175e-14,\n",
       "              6.2777e-13, -1.0585e-03,  1.9549e-12,  6.9780e-14,  0.0000e+00,\n",
       "              9.4832e-15,  8.9982e-15, -9.4508e-04, -1.6584e-03,  3.1841e-15,\n",
       "              7.9066e-05,  2.1078e-05,  2.1215e-12,  1.9613e-13,  2.3619e-13,\n",
       "              4.5650e-14,  4.1450e-13,  2.1417e-13,  1.1827e-03,  1.5445e-13,\n",
       "             -1.0061e-03,  1.6086e-13, -1.6074e-04,  9.0538e-05,  4.5250e-14,\n",
       "              7.9527e-04,  7.0855e-04,  1.6598e-14,  4.0421e-14, -1.2597e-03,\n",
       "              6.8796e-04,  1.0580e-12,  8.2250e-04,  7.7607e-04,  2.0841e-13,\n",
       "              1.2838e-03,  1.8856e-03,  6.7460e-13,  4.8202e-12,  4.1630e-04,\n",
       "             -9.1758e-06,  8.9347e-13,  1.1853e-13, -1.9324e-03,  5.3094e-13,\n",
       "              1.1798e-14,  7.6832e-04,  5.0643e-04,  6.3707e-04,  5.0252e-04,\n",
       "             -5.6775e-04,  2.0048e-13,  1.0897e-12,  3.7288e-16,  4.9038e-12,\n",
       "              4.9630e-14,  6.6661e-14,  5.0467e-12,  6.2869e-13, -1.7487e-03,\n",
       "              1.4691e-15,  7.9843e-04,  9.7303e-12, -1.2379e-03,  1.3746e-13,\n",
       "              5.0388e-15,  8.0356e-05,  2.7430e-14,  8.7950e-04,  1.3217e-13,\n",
       "              1.6180e-14,  1.5857e-03,  2.5219e-12, -1.1580e-03,  4.6141e-12,\n",
       "              3.3873e-14,  3.8037e-12,  2.1675e-12,  2.2554e-13,  8.1303e-12,\n",
       "             -1.5119e-03,  1.2704e-14,  1.9638e-04,  0.0000e+00,  7.6862e-04,\n",
       "              1.7132e-15,  6.2213e-13,  1.1684e-03,  6.0707e-14, -1.1041e-04,\n",
       "              1.6296e-03, -2.0278e-03,  3.8200e-12,  4.7352e-11,  1.1667e-12,\n",
       "              2.6792e-14,  7.2526e-14,  1.0484e-03,  4.9180e-12,  0.0000e+00,\n",
       "              4.6885e-12, -4.5940e-04,  2.2826e-13,  1.9817e-13, -5.7380e-04,\n",
       "              6.8146e-04, -2.4004e-04,  3.8540e-13,  3.4889e-04, -2.0122e-03,\n",
       "              2.7615e-07,  1.3676e-13,  2.6439e-13,  2.0336e-12,  1.4555e-03,\n",
       "              1.0730e-13,  1.4926e-13,  2.4276e-11, -3.0267e-04,  3.2323e-13,\n",
       "              8.7186e-15,  6.0079e-12,  1.6739e-12,  3.7122e-12,  1.3927e-03,\n",
       "              2.2830e-13,  3.7048e-12,  4.1440e-14,  9.6594e-04,  1.7827e-14,\n",
       "              4.2624e-13,  2.5312e-14,  9.2834e-04,  6.2374e-14, -3.5571e-05,\n",
       "              6.3667e-13,  9.8844e-12,  1.7566e-13,  1.5595e-03,  1.0842e-03,\n",
       "              1.5101e-14,  2.6049e-13,  3.0107e-15, -1.3174e-03, -1.5610e-04,\n",
       "              9.2064e-14,  9.4664e-04,  2.7915e-14,  7.1121e-16,  1.4697e-13,\n",
       "              2.7721e-12,  3.4075e-04,  8.9157e-04,  5.4085e-04,  2.1594e-13,\n",
       "              3.3418e-12,  9.8769e-04,  3.8824e-04,  3.2984e-12,  1.1958e-13,\n",
       "              7.0700e-12,  1.0126e-13,  4.9000e-13,  3.1262e-12,  1.5919e-12,\n",
       "              7.0784e-14, -7.6132e-04,  7.5401e-04,  2.5428e-12,  2.9920e-12,\n",
       "              5.7513e-12,  4.7807e-12,  1.1166e-03,  3.5111e-04,  8.5073e-14,\n",
       "              1.9906e-14, -9.1865e-05,  3.7651e-12,  9.2706e-04, -1.6926e-03,\n",
       "              8.7590e-13,  3.6366e-12,  8.1331e-13, -6.5528e-04,  3.6649e-14,\n",
       "              3.4564e-05,  8.9278e-15,  6.1986e-15,  2.0645e-16, -8.0291e-05,\n",
       "             -9.2022e-04,  1.0339e-03,  6.8569e-04,  6.5659e-14,  9.2168e-04,\n",
       "              3.2019e-12,  1.8476e-12,  6.4560e-10,  5.0619e-14,  8.5240e-13,\n",
       "              0.0000e+00,  2.6781e-12, -1.9604e-04,  3.1434e-14,  2.7024e-14,\n",
       "              9.1271e-13,  1.1530e-05,  5.4269e-14,  6.2046e-14, -1.0709e-03,\n",
       "              9.7536e-15,  2.4808e-13,  1.4058e-12,  1.1084e-05,  4.4652e-13,\n",
       "             -8.7823e-05,  1.5190e-03, -1.3940e-03,  2.7119e-13,  3.3289e-12,\n",
       "              1.0337e-14,  3.0300e-14,  1.0006e-14,  6.4672e-15,  4.0461e-12]),\n",
       "     'exp_avg_sq': tensor([4.4841e-08, 1.0896e-09, 4.6155e-07, 2.0583e-09, 1.2539e-06, 2.4620e-11,\n",
       "             3.0203e-14, 3.0673e-16, 1.1824e-06, 5.0428e-09, 3.4897e-05, 9.2739e-05,\n",
       "             3.4260e-05, 1.7392e-04, 1.2122e-04, 2.2841e-06, 2.6056e-10, 3.2166e-07,\n",
       "             8.0801e-09, 2.9485e-06, 1.4643e-08, 4.0997e-07, 4.3946e-07, 6.7914e-06,\n",
       "             5.6974e-07, 1.8875e-08, 2.0146e-09, 1.3175e-10, 9.7425e-05, 1.5955e-09,\n",
       "             7.6151e-07, 5.8489e-08, 4.8963e-06, 8.4317e-08, 3.8820e-08, 4.6224e-08,\n",
       "             1.7977e-04, 3.6512e-12, 2.5871e-09, 1.5889e-06, 2.9586e-09, 2.2172e-09,\n",
       "             4.0132e-08, 8.4060e-13, 7.2845e-06, 2.2052e-04, 1.3864e-08, 7.9952e-07,\n",
       "             9.0437e-13, 4.3875e-08, 2.4641e-05, 4.8331e-05, 3.4869e-06, 2.8086e-09,\n",
       "             3.1758e-08, 3.1804e-14, 1.1338e-12, 1.0380e-06, 8.0345e-09, 1.6043e-08,\n",
       "             1.1490e-07, 7.9890e-05, 1.3020e-07, 1.7591e-09, 3.5272e-05, 1.8301e-05,\n",
       "             1.8610e-05, 3.4563e-07, 7.4428e-11, 1.3911e-06, 5.6132e-05, 2.7625e-07,\n",
       "             1.3479e-10, 4.4485e-08, 2.0466e-10, 1.3505e-08, 7.2810e-05, 4.8544e-08,\n",
       "             1.3442e-09, 0.0000e+00, 1.1993e-11, 5.6915e-12, 4.2855e-05, 1.7497e-04,\n",
       "             3.6644e-13, 1.3701e-05, 1.0758e-07, 1.2270e-07, 3.3724e-09, 7.4393e-09,\n",
       "             1.4814e-10, 1.1836e-08, 6.1322e-09, 4.6550e-05, 3.1814e-09, 4.7191e-05,\n",
       "             2.2685e-09, 4.1429e-06, 4.5864e-06, 2.7306e-10, 4.1207e-05, 2.9630e-05,\n",
       "             2.9790e-11, 1.7667e-10, 8.8416e-05, 5.0727e-05, 4.8218e-08, 7.0707e-05,\n",
       "             2.4973e-05, 5.7922e-09, 5.3814e-05, 6.8702e-05, 6.9227e-09, 7.3565e-07,\n",
       "             5.9125e-05, 1.8876e-06, 3.5121e-08, 1.8736e-09, 2.7031e-04, 1.6366e-08,\n",
       "             1.8482e-12, 2.6408e-05, 1.5691e-05, 2.8284e-05, 1.4277e-05, 1.3016e-05,\n",
       "             5.3599e-09, 8.9435e-08, 6.4976e-15, 1.0727e-06, 3.2848e-10, 2.3168e-10,\n",
       "             2.8788e-07, 1.9483e-08, 1.6500e-04, 1.2440e-13, 3.2545e-05, 1.8110e-06,\n",
       "             1.1005e-04, 2.5200e-09, 6.3246e-13, 4.2289e-06, 1.0034e-10, 4.1609e-05,\n",
       "             2.3295e-09, 2.8307e-11, 1.5964e-04, 1.1942e-08, 9.3607e-05, 2.3978e-06,\n",
       "             1.5302e-10, 1.2936e-06, 5.9727e-07, 5.5003e-09, 3.0591e-07, 1.7930e-04,\n",
       "             7.5099e-13, 5.1522e-06, 0.0000e+00, 4.1693e-05, 3.1736e-13, 3.0119e-08,\n",
       "             4.1575e-05, 4.9146e-10, 9.3814e-07, 5.8276e-05, 3.2185e-04, 1.6037e-06,\n",
       "             3.1300e-07, 3.4862e-09, 9.5725e-11, 7.0147e-10, 4.3596e-05, 2.0406e-06,\n",
       "             0.0000e+00, 2.2098e-06, 1.3514e-06, 9.7816e-10, 5.2373e-09, 1.0102e-05,\n",
       "             1.9674e-05, 3.4655e-06, 1.3495e-08, 2.5256e-05, 2.6387e-04, 3.2882e-07,\n",
       "             2.4943e-09, 5.8588e-09, 3.9110e-07, 1.5597e-04, 1.5355e-09, 1.4134e-09,\n",
       "             2.7700e-07, 5.1047e-06, 7.1394e-09, 1.0137e-11, 6.5358e-06, 6.6098e-07,\n",
       "             2.0668e-06, 1.0905e-04, 6.9504e-09, 1.2276e-06, 4.2407e-13, 5.3306e-05,\n",
       "             4.2383e-11, 2.4229e-08, 8.5441e-11, 3.9180e-05, 2.7298e-13, 1.1879e-06,\n",
       "             5.4056e-08, 1.4308e-06, 4.1151e-09, 7.1116e-05, 4.6911e-05, 1.6211e-11,\n",
       "             8.7289e-09, 6.4432e-13, 1.0058e-04, 5.3683e-06, 1.1303e-09, 5.0952e-05,\n",
       "             2.8908e-11, 3.5956e-14, 2.8806e-09, 7.1639e-07, 9.8083e-06, 4.9476e-05,\n",
       "             2.6915e-05, 6.2185e-09, 1.2961e-06, 6.4638e-05, 1.1475e-05, 1.9050e-06,\n",
       "             7.2503e-10, 2.2332e-06, 1.3674e-09, 1.4562e-08, 2.4526e-07, 2.1879e-07,\n",
       "             6.6816e-10, 3.4278e-05, 5.8605e-05, 8.0887e-07, 1.0245e-07, 5.2279e-07,\n",
       "             2.1354e-06, 6.8903e-06, 2.9638e-05, 9.6516e-10, 5.2841e-11, 2.6295e-06,\n",
       "             1.5117e-06, 6.0243e-05, 2.1581e-04, 1.8833e-08, 1.2167e-06, 1.6733e-08,\n",
       "             2.0677e-05, 1.7912e-10, 8.9932e-06, 5.6414e-13, 2.7313e-12, 5.6838e-15,\n",
       "             1.8168e-06, 3.4553e-05, 4.5660e-05, 3.7714e-05, 5.7492e-10, 4.4034e-05,\n",
       "             9.3711e-07, 4.0311e-08, 8.5056e-07, 3.4170e-10, 6.6475e-08, 0.0000e+00,\n",
       "             6.2781e-07, 1.9440e-06, 1.3177e-10, 9.7394e-11, 6.9442e-09, 4.9903e-06,\n",
       "             3.9276e-10, 5.1339e-10, 5.2396e-05, 8.3405e-12, 3.9651e-09, 2.1481e-07,\n",
       "             1.4680e-06, 2.1670e-08, 2.9924e-06, 1.8300e-04, 1.3940e-04, 9.8078e-09,\n",
       "             8.1865e-07, 1.4251e-11, 1.2243e-10, 1.3351e-11, 1.1776e-11, 1.4747e-06])},\n",
       "    6: {'step': tensor(200.),\n",
       "     'exp_avg': tensor([[ 3.7309e-15,  6.4183e-16,  6.2137e-13,  ...,  2.1522e-16,\n",
       "               5.6451e-16,  4.2593e-13],\n",
       "             [-2.0048e-13,  0.0000e+00, -1.6491e-13,  ...,  0.0000e+00,\n",
       "               0.0000e+00,  5.9295e-15],\n",
       "             [ 1.0333e-13, -1.3857e-15,  3.0861e-13,  ..., -5.0895e-16,\n",
       "              -1.0251e-15, -4.4710e-14],\n",
       "             ...,\n",
       "             [-5.3334e-14, -3.9062e-17,  8.0256e-13,  ...,  5.8257e-16,\n",
       "               5.3603e-16,  5.9561e-13],\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "               0.0000e+00,  0.0000e+00],\n",
       "             [-2.0543e-15,  7.9291e-16,  4.7614e-13,  ...,  3.7906e-16,\n",
       "               2.9625e-16,  1.8641e-13]]),\n",
       "     'exp_avg_sq': tensor([[4.6317e-12, 3.0999e-14, 7.4590e-08,  ..., 6.1771e-15, 2.0756e-14,\n",
       "              1.6317e-08],\n",
       "             [8.1905e-10, 0.0000e+00, 3.5332e-10,  ..., 0.0000e+00, 0.0000e+00,\n",
       "              2.4992e-12],\n",
       "             [7.9402e-10, 1.6230e-13, 3.8774e-09,  ..., 3.4544e-14, 6.3652e-14,\n",
       "              3.4430e-10],\n",
       "             ...,\n",
       "             [4.1958e-10, 2.2207e-14, 1.7963e-07,  ..., 4.5260e-14, 2.8350e-14,\n",
       "              3.9233e-08],\n",
       "             [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "              0.0000e+00],\n",
       "             [5.6278e-13, 8.3842e-14, 1.9847e-08,  ..., 1.9162e-14, 1.1704e-14,\n",
       "              3.7017e-09]])},\n",
       "    7: {'step': tensor(200.),\n",
       "     'exp_avg': tensor([ 2.1969e-12,  8.2573e-04, -2.3473e-04,  3.8200e-04,  4.4433e-04,\n",
       "              3.7584e-12,  3.4405e-12,  4.6030e-04, -5.5405e-04,  1.2076e-05,\n",
       "              9.1485e-13,  2.4885e-12,  1.9728e-12,  3.7884e-04, -6.1632e-04,\n",
       "              2.2706e-12,  3.2350e-13,  5.6609e-04,  1.9844e-13,  1.0857e-12,\n",
       "              8.8938e-16,  3.7511e-13,  7.8689e-13,  2.6122e-04,  3.1964e-13,\n",
       "              2.2498e-04, -1.9642e-04,  0.0000e+00,  3.5109e-11, -8.7623e-04,\n",
       "              3.3621e-04,  6.5306e-05,  0.0000e+00,  3.6780e-12,  1.6200e-12,\n",
       "              4.4332e-04,  1.8328e-04,  1.3964e-14, -6.4205e-04,  1.9021e-04,\n",
       "              1.8404e-13,  3.0863e-06,  3.6702e-12,  4.7332e-04,  7.0224e-04,\n",
       "              2.6459e-13,  3.8549e-04, -6.5203e-05,  2.7163e-12,  5.4671e-13,\n",
       "              1.0911e-12,  2.6244e-04,  3.7457e-04,  8.4899e-12,  1.1413e-12,\n",
       "              1.0108e-10,  3.8407e-12,  3.2087e-12, -3.6320e-04,  4.2877e-13,\n",
       "              2.5281e-04,  2.4611e-10,  4.3589e-12,  8.3587e-13,  5.1265e-12,\n",
       "              6.0957e-04,  1.3203e-04,  3.9762e-15,  4.6420e-13,  1.0235e-12,\n",
       "             -4.2315e-04,  1.5461e-04,  8.7782e-13,  3.8239e-12,  9.3221e-13,\n",
       "              5.1052e-13,  2.5834e-12,  2.7751e-13,  0.0000e+00,  0.0000e+00,\n",
       "              1.5198e-12,  1.9601e-12,  1.2144e-03,  3.2684e-04, -2.8412e-04,\n",
       "              9.4213e-13, -3.1360e-04,  1.9909e-12, -1.4897e-04,  5.2458e-13,\n",
       "             -3.1189e-04,  5.0422e-13,  1.2336e-12,  2.2568e-04,  3.8822e-12,\n",
       "              6.8213e-12,  2.4455e-04,  1.1410e-12,  1.1518e-12,  1.5561e-12,\n",
       "             -5.6121e-04,  1.5049e-05,  3.5518e-13,  5.5206e-13,  3.1428e-04,\n",
       "              0.0000e+00,  2.4966e-04,  1.7902e-12,  0.0000e+00, -4.5601e-04,\n",
       "              2.3872e-12,  1.2749e-12,  5.3987e-11,  1.1564e-13,  5.1058e-04,\n",
       "             -4.7434e-04, -6.7883e-05,  1.0768e-14,  4.8743e-14,  8.7057e-14,\n",
       "             -4.6835e-04,  6.4737e-13,  0.0000e+00,  9.8678e-13,  7.3431e-13,\n",
       "             -1.0298e-04,  6.6857e-04, -9.5950e-04,  0.0000e+00,  1.1703e-13,\n",
       "              7.3678e-12,  1.3154e-03,  5.0272e-04,  5.1553e-13,  0.0000e+00,\n",
       "              3.0765e-04,  1.9985e-13,  2.0097e-12,  1.7941e-04,  4.2295e-13,\n",
       "              2.2326e-12,  0.0000e+00,  4.9610e-04,  1.0420e-03,  2.1644e-13,\n",
       "             -1.6042e-04,  5.2530e-12, -3.1242e-04,  5.6132e-04,  1.0724e-03,\n",
       "              5.1970e-13,  1.0045e-04,  6.3428e-12,  0.0000e+00, -3.4451e-04,\n",
       "              4.4226e-12, -6.7160e-04,  6.0854e-12, -9.2379e-04,  5.9904e-13,\n",
       "              5.9732e-04,  0.0000e+00,  5.7222e-12, -5.7601e-04,  4.5205e-12,\n",
       "             -3.9927e-04,  3.2946e-13,  4.8458e-12,  1.9022e-13,  1.6034e-13,\n",
       "              1.6048e-12,  8.4680e-13,  2.7092e-13,  2.7785e-12,  2.2856e-04,\n",
       "              1.8326e-12,  1.1228e-12, -3.0399e-04,  7.0432e-13,  7.3845e-04,\n",
       "              1.1683e-12, -1.5905e-04,  9.5164e-13,  0.0000e+00,  1.7763e-04,\n",
       "             -6.7973e-04,  7.0672e-04,  2.3385e-13,  1.1180e-12,  7.8432e-04,\n",
       "              3.8342e-13, -7.0840e-05,  3.8829e-12, -3.1720e-04,  2.9103e-13,\n",
       "             -4.6990e-04,  3.2237e-12,  2.2048e-04,  2.8470e-14,  5.0561e-12,\n",
       "              1.6232e-13,  4.5475e-12, -1.5801e-04, -4.5869e-04, -6.9003e-04,\n",
       "             -5.3982e-04,  3.3664e-13,  1.2353e-12,  2.5579e-13,  5.8450e-04,\n",
       "             -3.0084e-04,  6.2287e-12,  3.0439e-12,  4.1988e-13,  2.1740e-04,\n",
       "              3.4386e-04,  7.8191e-14,  3.2408e-12, -2.5784e-04,  0.0000e+00,\n",
       "              1.4649e-12,  5.3846e-14,  2.2674e-04, -3.4381e-04,  1.3945e-13,\n",
       "              2.0465e-04,  5.5316e-13, -4.3158e-04,  6.6349e-14,  4.5100e-04,\n",
       "              5.6979e-13,  7.9833e-05,  5.0022e-15, -4.7847e-04,  9.0708e-05,\n",
       "             -3.5975e-04,  2.1861e-04,  3.9428e-04, -4.6996e-04,  6.2175e-04,\n",
       "              8.5233e-13,  3.8258e-13,  3.3195e-04,  1.3014e-13,  1.3270e-12,\n",
       "              3.4068e-04,  4.3750e-04,  2.1229e-04,  7.6369e-13,  1.0529e-12,\n",
       "              2.5280e-04,  1.3549e-13,  5.9044e-05,  5.5552e-04,  4.9871e-04,\n",
       "             -7.4275e-04,  7.2113e-13,  6.4584e-04,  4.4320e-11,  6.8666e-14,\n",
       "              2.2341e-13, -6.0963e-04,  7.4509e-13,  6.6097e-04, -3.2457e-04,\n",
       "             -4.2758e-04, -4.5947e-04,  2.6457e-04,  6.9811e-13,  3.3931e-13,\n",
       "              0.0000e+00, -9.3912e-05, -1.7229e-04,  1.8791e-12,  0.0000e+00,\n",
       "              6.8912e-14,  1.7295e-04,  7.3331e-13,  1.3813e-04,  1.3629e-12,\n",
       "             -3.1921e-04, -6.5575e-04,  9.3213e-13,  5.1398e-13,  2.7376e-13,\n",
       "              1.7037e-04, -7.1240e-04,  1.0253e-14,  5.0451e-13,  3.1223e-14,\n",
       "              2.0369e-12,  1.6246e-12,  8.4006e-05,  3.3934e-12,  4.3765e-14,\n",
       "              0.0000e+00,  0.0000e+00,  2.0523e-04,  0.0000e+00,  1.7307e-12]),\n",
       "     'exp_avg_sq': tensor([1.0186e-06, 4.6320e-05, 4.6702e-06, 1.1429e-05, 2.2633e-05, 2.8420e-06,\n",
       "             3.8702e-06, 4.0819e-06, 2.0942e-05, 3.3636e-07, 1.1161e-07, 5.4293e-07,\n",
       "             7.3656e-07, 4.8199e-06, 2.4789e-05, 3.7023e-06, 6.0316e-09, 3.0138e-06,\n",
       "             3.3621e-09, 8.2124e-08, 5.6228e-14, 8.3355e-09, 3.5055e-07, 7.3144e-06,\n",
       "             1.3625e-08, 1.0904e-06, 4.6409e-06, 0.0000e+00, 6.2733e-06, 4.4571e-05,\n",
       "             9.9946e-06, 7.0308e-07, 0.0000e+00, 2.7101e-06, 5.0259e-07, 1.8909e-05,\n",
       "             6.6469e-06, 2.6005e-11, 2.7064e-05, 1.0287e-06, 4.4560e-09, 2.9736e-07,\n",
       "             1.1766e-06, 1.1963e-05, 1.2009e-05, 9.3361e-09, 7.7727e-06, 9.7665e-07,\n",
       "             1.1491e-06, 1.8740e-08, 1.2872e-07, 1.4448e-05, 7.5646e-06, 6.3050e-06,\n",
       "             1.7370e-07, 3.8742e-08, 1.2905e-06, 1.1714e-06, 2.1368e-05, 2.4516e-08,\n",
       "             1.2711e-05, 2.0736e-06, 2.0279e-09, 8.2266e-08, 3.4881e-07, 8.7560e-06,\n",
       "             2.9174e-06, 2.0902e-14, 7.8433e-09, 1.1327e-07, 1.4240e-05, 9.4018e-07,\n",
       "             3.0847e-07, 2.8928e-06, 2.8092e-08, 2.8182e-08, 5.1314e-08, 8.3269e-09,\n",
       "             0.0000e+00, 0.0000e+00, 1.4402e-07, 3.1817e-07, 1.0507e-04, 2.0984e-05,\n",
       "             6.4182e-06, 2.8345e-07, 8.3092e-06, 5.6767e-07, 4.1382e-06, 9.5178e-09,\n",
       "             7.0453e-06, 9.7276e-09, 1.5524e-07, 5.5177e-06, 3.5726e-06, 4.0701e-06,\n",
       "             4.7167e-06, 1.7360e-07, 2.2290e-07, 3.2293e-07, 3.3759e-05, 7.9260e-07,\n",
       "             1.6824e-08, 1.7566e-08, 7.2198e-06, 0.0000e+00, 1.9181e-05, 5.2665e-07,\n",
       "             0.0000e+00, 2.0186e-05, 4.0820e-08, 1.7576e-07, 8.3049e-08, 7.7075e-10,\n",
       "             1.0033e-05, 2.1893e-05, 2.1501e-06, 1.5463e-11, 7.2994e-11, 1.0107e-09,\n",
       "             1.3436e-05, 2.5142e-08, 0.0000e+00, 1.2220e-07, 3.7467e-08, 2.4993e-06,\n",
       "             2.8291e-05, 6.1126e-05, 0.0000e+00, 1.8264e-09, 7.0400e-06, 9.5553e-05,\n",
       "             1.3958e-05, 5.0405e-08, 0.0000e+00, 1.1815e-05, 6.5410e-10, 5.1816e-07,\n",
       "             8.1686e-06, 2.3856e-08, 8.6227e-07, 0.0000e+00, 2.6736e-05, 6.1602e-05,\n",
       "             2.5660e-09, 2.4570e-06, 4.5422e-06, 4.8653e-06, 2.6966e-05, 1.3675e-05,\n",
       "             1.7388e-08, 1.5785e-06, 3.5271e-06, 0.0000e+00, 8.1350e-06, 3.0288e-06,\n",
       "             2.9615e-05, 7.7039e-06, 5.3459e-05, 4.7236e-11, 1.6619e-05, 0.0000e+00,\n",
       "             3.5328e-07, 2.2420e-05, 1.7915e-06, 1.1560e-05, 6.5325e-09, 5.4054e-12,\n",
       "             2.0855e-09, 3.4284e-09, 2.2519e-07, 9.5627e-08, 7.9366e-09, 1.6149e-06,\n",
       "             6.5767e-06, 3.1004e-07, 9.0804e-08, 7.7071e-06, 4.6550e-09, 1.1151e-05,\n",
       "             7.0283e-08, 5.7716e-06, 4.3363e-08, 0.0000e+00, 4.3587e-06, 2.8387e-05,\n",
       "             9.3821e-06, 7.2928e-09, 1.6668e-07, 5.9291e-05, 1.9605e-08, 5.3374e-07,\n",
       "             4.4272e-06, 1.0823e-05, 9.1580e-09, 1.7355e-05, 1.4472e-06, 8.5103e-07,\n",
       "             8.6101e-11, 3.1516e-07, 1.3190e-09, 4.6594e-06, 7.5217e-06, 2.2574e-05,\n",
       "             3.0923e-05, 1.8193e-05, 2.9338e-09, 2.9850e-08, 2.4791e-09, 3.2394e-06,\n",
       "             1.0464e-05, 3.3906e-06, 4.3110e-08, 2.6032e-09, 9.6128e-07, 5.2183e-06,\n",
       "             3.5238e-10, 1.0231e-06, 1.1305e-05, 0.0000e+00, 1.6191e-07, 1.5329e-10,\n",
       "             5.2630e-06, 1.8695e-05, 3.9278e-10, 6.0458e-06, 8.8348e-09, 1.2466e-05,\n",
       "             5.8707e-10, 2.2618e-05, 4.3137e-09, 5.8325e-06, 6.2331e-13, 2.1219e-05,\n",
       "             1.0431e-06, 8.4682e-06, 8.0197e-06, 2.9525e-06, 1.4641e-05, 7.8189e-06,\n",
       "             8.4275e-08, 1.9520e-08, 3.0465e-05, 1.3180e-09, 9.7113e-08, 1.2171e-05,\n",
       "             1.5811e-05, 9.3481e-06, 3.0243e-08, 4.2156e-08, 7.7995e-06, 1.6095e-09,\n",
       "             1.9436e-06, 2.7542e-05, 2.1536e-05, 4.9939e-05, 1.8951e-09, 5.1455e-05,\n",
       "             6.9692e-09, 5.0982e-10, 6.6560e-09, 3.6654e-05, 7.4036e-08, 4.3670e-05,\n",
       "             1.2392e-05, 1.1013e-05, 2.7423e-05, 7.1972e-06, 6.4992e-08, 1.0094e-08,\n",
       "             0.0000e+00, 5.8638e-06, 6.6455e-06, 3.0821e-07, 0.0000e+00, 6.3331e-10,\n",
       "             2.7254e-06, 2.8116e-08, 5.4813e-06, 9.6333e-08, 7.3740e-06, 2.6614e-05,\n",
       "             5.7429e-08, 1.6520e-08, 2.5691e-09, 2.8067e-06, 3.3494e-05, 4.9129e-12,\n",
       "             2.2315e-08, 1.3001e-10, 1.4898e-07, 5.4137e-07, 3.5960e-06, 2.9265e-06,\n",
       "             2.5543e-10, 0.0000e+00, 0.0000e+00, 6.2787e-06, 0.0000e+00, 2.6121e-07])},\n",
       "    8: {'step': tensor(200.),\n",
       "     'exp_avg': tensor([[ 5.5474e-11,  1.1064e-02,  4.2463e-03,  1.0761e-04,  5.6998e-03,\n",
       "               3.5765e-11,  2.9730e-11,  3.5766e-03,  4.5256e-03,  4.6874e-04,\n",
       "              -4.1050e-14,  4.4487e-12,  2.1039e-11,  3.5130e-03,  5.0304e-03,\n",
       "               4.3585e-11, -1.1948e-13, -3.4654e-04, -6.6974e-13, -3.3081e-12,\n",
       "              -2.9177e-15, -7.3750e-13, -8.3808e-12,  5.2485e-03,  8.3757e-13,\n",
       "               2.1836e-03,  1.7610e-03,  0.0000e+00, -5.8139e-11,  5.7418e-03,\n",
       "               2.3189e-03,  2.5984e-03,  0.0000e+00,  3.7956e-11,  3.7486e-11,\n",
       "               6.3400e-03,  3.4962e-04,  9.8913e-14,  4.8272e-03,  1.1878e-04,\n",
       "               7.3183e-14, -6.0788e-05,  8.7246e-12,  7.3075e-03,  3.9472e-03,\n",
       "               2.1738e-13,  2.3650e-04,  1.4938e-03,  4.5638e-11, -3.0793e-12,\n",
       "              -4.0560e-13,  2.5139e-04,  7.9442e-03,  1.6478e-11,  5.7143e-13,\n",
       "               1.2827e-09,  2.2405e-12,  1.8645e-11,  4.4553e-03,  5.2342e-13,\n",
       "               3.6539e-04, -9.5146e-10, -1.9805e-12,  2.7145e-11,  4.0651e-12,\n",
       "               4.6059e-03, -4.1069e-05, -1.7459e-15, -2.1248e-12, -5.8595e-13,\n",
       "               3.6670e-03,  3.6085e-05,  2.6372e-11,  2.4680e-11,  2.9071e-13,\n",
       "              -1.2419e-12,  5.5578e-12,  3.8262e-14,  0.0000e+00,  0.0000e+00,\n",
       "               2.5462e-14, -8.5696e-13,  1.1960e-02,  4.8825e-04,  3.1959e-03,\n",
       "              -7.4837e-12,  2.6240e-03,  1.6928e-11,  3.6424e-03,  7.0927e-13,\n",
       "               4.1503e-03,  5.0914e-12, -8.8049e-13,  4.8962e-03,  2.6122e-11,\n",
       "               1.2596e-11,  4.4910e-03, -1.4120e-13,  4.1163e-11, -4.5301e-14,\n",
       "               5.7454e-03,  1.9750e-03,  6.8093e-13, -5.2488e-14,  3.8086e-03,\n",
       "               0.0000e+00,  4.6474e-04, -3.0463e-12,  0.0000e+00,  4.9683e-03,\n",
       "              -3.8992e-13, -4.9699e-13, -2.5211e-10, -2.2355e-13,  3.0761e-04,\n",
       "               4.8621e-03,  3.1093e-03,  1.0487e-14,  9.9719e-16,  8.4521e-14,\n",
       "               4.9265e-03, -2.1393e-12,  0.0000e+00,  1.3632e-11, -1.3932e-12,\n",
       "               3.3749e-03,  8.5880e-03,  5.5000e-03,  0.0000e+00,  3.3019e-13,\n",
       "               8.5625e-12,  1.2928e-02,  7.3720e-03, -3.0856e-12,  0.0000e+00,\n",
       "               3.9917e-03,  1.9020e-14,  1.3575e-11,  3.0385e-04,  1.1669e-13,\n",
       "               3.1210e-11,  0.0000e+00,  4.6955e-03,  1.2314e-02, -1.3742e-12,\n",
       "               3.6496e-03,  3.1283e-11,  1.5680e-03,  6.5169e-03,  1.6580e-04,\n",
       "              -1.2288e-12,  3.5343e-03,  3.3503e-12,  0.0000e+00,  4.2966e-03,\n",
       "              -3.0952e-13,  6.6932e-03,  3.1980e-11,  2.2946e-03, -1.4060e-14,\n",
       "               1.0146e-02,  0.0000e+00,  3.6806e-12,  4.9096e-03,  7.9535e-13,\n",
       "               4.3993e-03,  1.9488e-13, -5.1060e-10, -6.8746e-14,  3.8235e-14,\n",
       "               4.1151e-13, -1.0066e-12, -7.7376e-13,  3.0632e-11,  3.7889e-04,\n",
       "               1.3897e-11,  8.1397e-13,  4.2346e-03, -2.1359e-15,  4.0937e-03,\n",
       "               2.0752e-11,  3.0155e-03,  7.2214e-13,  0.0000e+00,  3.1888e-05,\n",
       "               4.5311e-03,  4.3304e-03,  1.7747e-13,  8.1999e-14,  1.4155e-03,\n",
       "              -5.4779e-14,  7.7719e-04,  4.7242e-11,  5.2897e-03, -1.1069e-12,\n",
       "               4.9776e-03,  2.5306e-11,  2.6283e-04,  6.2413e-14,  4.0623e-12,\n",
       "               1.2239e-12,  4.2131e-11,  3.7211e-03,  5.1159e-03,  4.9890e-03,\n",
       "               4.3828e-03,  5.2963e-14,  7.5731e-13,  1.1345e-14,  2.4766e-04,\n",
       "               3.6670e-03,  1.7106e-11, -3.7768e-13, -4.8009e-13,  2.3049e-04,\n",
       "               2.5610e-03, -1.8665e-13,  1.1410e-12,  2.7198e-03,  0.0000e+00,\n",
       "              -1.0306e-12, -6.7283e-14,  3.0444e-05,  5.2086e-03,  1.7616e-14,\n",
       "               4.0324e-03,  1.6609e-14,  4.3215e-03,  9.0419e-14,  5.5283e-03,\n",
       "               3.4655e-12,  2.0726e-03,  2.3094e-15,  5.9351e-03,  1.3894e-04,\n",
       "               4.4065e-03,  3.5449e-04,  3.9950e-03,  4.8099e-03,  4.0381e-03,\n",
       "               1.3446e-11, -2.2433e-14,  2.7486e-04, -2.3358e-13, -1.1442e-12,\n",
       "               4.8356e-03,  5.5079e-03,  2.3362e-04, -7.7675e-13, -8.1267e-13,\n",
       "               3.8880e-03, -2.0693e-13, -2.8775e-05,  6.3854e-03,  6.8528e-03,\n",
       "               7.8731e-03, -2.4194e-12,  1.3236e-03,  1.8246e-10,  1.8239e-13,\n",
       "               8.9866e-15,  5.5554e-03,  7.5753e-13,  6.3857e-03,  4.4536e-03,\n",
       "               5.1424e-03,  3.6467e-03,  5.1031e-03,  5.9385e-14,  2.2190e-14,\n",
       "               0.0000e+00,  6.1759e-04,  3.5113e-03,  1.2243e-11,  0.0000e+00,\n",
       "               8.5512e-13, -1.3418e-04, -1.5568e-12,  1.3009e-03, -2.7972e-12,\n",
       "               4.4502e-03,  5.4526e-03,  4.9258e-14, -9.3118e-13, -3.6006e-12,\n",
       "               2.2805e-04,  7.3571e-03,  3.0377e-17, -2.9844e-13,  1.1239e-14,\n",
       "              -1.6976e-12,  3.1837e-11,  5.9683e-04,  4.3698e-11,  4.5020e-15,\n",
       "               0.0000e+00,  0.0000e+00,  3.7876e-03,  0.0000e+00,  3.7121e-12]]),\n",
       "     'exp_avg_sq': tensor([[3.7355e-04, 5.1593e-03, 8.2591e-04, 5.2064e-04, 2.3048e-03, 1.6033e-04,\n",
       "              9.7137e-05, 2.9297e-04, 1.2991e-03, 1.5216e-04, 2.2473e-10, 1.7351e-06,\n",
       "              4.1691e-05, 2.9280e-04, 1.5560e-03, 2.4108e-04, 8.2285e-10, 6.1105e-05,\n",
       "              5.6805e-08, 1.0025e-06, 6.0512e-13, 4.6689e-07, 1.4869e-04, 1.7589e-03,\n",
       "              9.3554e-08, 1.4835e-04, 1.9480e-04, 0.0000e+00, 8.4302e-05, 2.1241e-03,\n",
       "              2.5913e-04, 4.9244e-04, 0.0000e+00, 1.6123e-04, 1.3135e-04, 2.5130e-03,\n",
       "              8.3522e-04, 1.3047e-09, 9.1629e-04, 2.8924e-04, 6.2868e-10, 9.7821e-05,\n",
       "              6.1419e-06, 2.2596e-03, 2.7174e-04, 6.3020e-09, 6.0509e-04, 5.7269e-04,\n",
       "              2.4772e-04, 6.4153e-07, 1.7788e-08, 5.8330e-04, 2.1951e-03, 2.5739e-05,\n",
       "              4.3545e-08, 4.8695e-08, 4.5980e-07, 2.8166e-05, 8.1703e-04, 3.6536e-08,\n",
       "              8.2409e-04, 7.1573e-05, 4.1863e-10, 7.7661e-05, 2.0598e-07, 4.4564e-04,\n",
       "              5.1193e-04, 4.0301e-15, 1.8703e-06, 3.7125e-08, 5.9286e-04, 5.0170e-04,\n",
       "              6.6942e-05, 5.5084e-05, 4.7215e-08, 1.6676e-07, 3.0244e-05, 1.5829e-10,\n",
       "              0.0000e+00, 0.0000e+00, 6.4050e-08, 5.3928e-08, 5.6604e-03, 8.8984e-04,\n",
       "              6.0170e-04, 1.9751e-04, 5.2151e-04, 2.9756e-05, 6.9374e-04, 1.0783e-08,\n",
       "              9.9419e-04, 1.3511e-06, 5.9056e-08, 1.3051e-03, 6.9317e-05, 1.5084e-05,\n",
       "              1.1825e-03, 2.6590e-09, 1.8062e-04, 2.7368e-10, 1.8585e-03, 7.7871e-04,\n",
       "              6.1834e-08, 1.5879e-10, 3.8517e-04, 0.0000e+00, 5.7619e-04, 1.3917e-05,\n",
       "              0.0000e+00, 1.4076e-03, 7.2356e-08, 2.6708e-08, 5.6207e-07, 2.8803e-09,\n",
       "              1.9555e-04, 1.7075e-03, 5.9868e-04, 1.4667e-11, 3.0550e-14, 9.5268e-10,\n",
       "              1.5203e-03, 2.7796e-07, 0.0000e+00, 1.6402e-05, 1.0729e-07, 7.3745e-04,\n",
       "              3.5778e-03, 1.5168e-03, 0.0000e+00, 1.4539e-08, 6.6650e-06, 6.0971e-03,\n",
       "              1.6538e-03, 2.8805e-07, 0.0000e+00, 7.7355e-04, 5.9244e-12, 1.5732e-05,\n",
       "              6.4861e-04, 1.8160e-09, 1.1012e-04, 0.0000e+00, 1.4333e-03, 5.4926e-03,\n",
       "              8.3332e-08, 8.0192e-04, 9.8105e-05, 1.0430e-04, 2.0116e-03, 9.7986e-05,\n",
       "              6.2450e-08, 4.6477e-04, 9.8407e-07, 0.0000e+00, 1.1937e-03, 6.8601e-07,\n",
       "              2.1320e-03, 1.1268e-04, 6.5474e-04, 2.6020e-14, 3.6616e-03, 0.0000e+00,\n",
       "              4.4992e-07, 1.1830e-03, 5.5459e-08, 1.1052e-03, 1.3210e-08, 5.0821e-08,\n",
       "              2.7239e-10, 1.9496e-10, 1.7027e-08, 1.3513e-07, 6.4737e-08, 9.6693e-05,\n",
       "              6.5185e-04, 1.7181e-05, 4.6911e-08, 8.0220e-04, 1.7381e-11, 3.6860e-04,\n",
       "              6.8355e-05, 5.0056e-04, 3.0569e-08, 0.0000e+00, 4.3857e-04, 1.5427e-03,\n",
       "              3.8264e-04, 4.2001e-09, 8.9668e-10, 9.9438e-04, 4.0018e-10, 5.7145e-06,\n",
       "              2.5243e-04, 1.3446e-03, 1.3247e-07, 1.5513e-03, 6.8167e-05, 1.4264e-04,\n",
       "              1.4045e-09, 4.4519e-07, 4.1174e-06, 2.1394e-04, 5.0912e-04, 1.4700e-03,\n",
       "              1.2525e-03, 1.3709e-03, 1.4605e-10, 3.9925e-08, 4.8767e-12, 2.3446e-05,\n",
       "              1.0728e-03, 2.4083e-05, 6.6372e-10, 1.1302e-06, 3.0595e-04, 1.3616e-04,\n",
       "              2.0080e-09, 1.1346e-07, 4.1101e-04, 0.0000e+00, 1.4347e-07, 2.4429e-10,\n",
       "              6.8633e-04, 8.2065e-04, 6.2677e-12, 9.2492e-04, 3.6110e-06, 1.3206e-03,\n",
       "              1.0903e-09, 1.5044e-03, 1.5615e-07, 1.6666e-04, 1.3285e-13, 1.6265e-03,\n",
       "              3.4151e-04, 1.1621e-03, 6.4310e-04, 3.4736e-04, 1.2723e-03, 3.8030e-04,\n",
       "              2.1787e-05, 6.7113e-11, 1.4087e-03, 4.0113e-09, 8.0223e-08, 1.5542e-03,\n",
       "              1.4302e-03, 7.3613e-04, 3.5302e-08, 5.1072e-08, 7.5022e-04, 3.7540e-09,\n",
       "              6.9888e-04, 2.1579e-03, 2.2732e-03, 3.1839e-03, 3.2778e-08, 1.3685e-03,\n",
       "              5.3371e-08, 3.5968e-09, 1.0770e-11, 1.8368e-03, 7.6527e-08, 1.9938e-03,\n",
       "              8.7829e-04, 1.8291e-03, 5.7781e-04, 1.5263e-03, 4.7029e-10, 4.3169e-11,\n",
       "              0.0000e+00, 1.2586e-05, 4.6116e-04, 1.2800e-05, 0.0000e+00, 9.7516e-08,\n",
       "              4.5950e-04, 1.2837e-07, 5.8535e-04, 3.8681e-07, 1.1314e-03, 2.1072e-03,\n",
       "              9.4279e-10, 7.6095e-08, 1.1499e-06, 7.7363e-04, 2.4395e-03, 4.3124e-17,\n",
       "              7.8084e-09, 1.6845e-11, 1.1119e-07, 1.1035e-04, 3.7316e-05, 2.2163e-04,\n",
       "              2.7029e-12, 0.0000e+00, 0.0000e+00, 7.0892e-04, 0.0000e+00, 8.5006e-07]])},\n",
       "    9: {'step': tensor(200.),\n",
       "     'exp_avg': tensor([0.0106]),\n",
       "     'exp_avg_sq': tensor([0.0098])}},\n",
       "   'param_groups': [{'lr': 0.005,\n",
       "     'betas': (0.9, 0.999),\n",
       "     'eps': 1e-08,\n",
       "     'weight_decay': 0,\n",
       "     'amsgrad': False,\n",
       "     'maximize': False,\n",
       "     'foreach': None,\n",
       "     'capturable': False,\n",
       "     'differentiable': False,\n",
       "     'fused': None,\n",
       "     'initial_lr': 0.005,\n",
       "     'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]}],\n",
       " 'lr_schedulers': [{'num_lrs': 1,\n",
       "   'warmup_epochs': array([2]),\n",
       "   'total_epochs': array([200]),\n",
       "   'steps_per_epoch': -1,\n",
       "   'init_lr': array([0.005]),\n",
       "   'max_lr': array([0.01]),\n",
       "   'final_lr': array([0.005]),\n",
       "   'current_step': 201,\n",
       "   'lr': [0.005],\n",
       "   'warmup_steps': array([-2]),\n",
       "   'total_steps': array([-200]),\n",
       "   'linear_increment': array([-0.0025]),\n",
       "   'gamma': array([1.00350688]),\n",
       "   'base_lrs': [0.005],\n",
       "   'last_epoch': -1,\n",
       "   'verbose': False,\n",
       "   '_step_count': 0}],\n",
       " 'hparams_name': 'kwargs',\n",
       " 'hyper_parameters': {'n_tasks': 1,\n",
       "  'ffn_hidden_dim': 300,\n",
       "  'ffn_num_layers': 1,\n",
       "  'dropout': 0.0,\n",
       "  'activation': 'relu',\n",
       "  'criterion': None,\n",
       "  'metrics': None,\n",
       "  'task_weights': None,\n",
       "  'warmup_epochs': 2,\n",
       "  'num_lrs': 1,\n",
       "  'init_lr': 0.005,\n",
       "  'max_lr': 0.01,\n",
       "  'final_lr': 0.005,\n",
       "  'scaler': StandardScaler()}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('/home/gridsan/nmorgan/chemprop/nathan/lightning_logs/version_23652061/checkpoints/epoch=199-step=200.ckpt')\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molenc_chk = modules.molecule_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnn_chk = models.RegressionMPNN(\n",
    "mpnn_chk.load_state_dict("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/nmorgan/mambaforge/envs/chemprop_gpu/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:196: UserWarning: Attribute 'mpn_block' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['mpn_block'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "mpnn_chk = models.RegressionMPNN.load_from_checkpoint('/home/gridsan/nmorgan/chemprop/nathan/lightning_logs/version_23656678/checkpoints/epoch=199-step=200.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-af0a01fe-fd4e-0ebe-1d1e-22a5448c6189]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(tensor([[133.2245]]), None),\n",
       " (tensor([[183.7518]]), None),\n",
       " (tensor([[426.3944]]), None),\n",
       " (tensor([[114.8588]]), None),\n",
       " (tensor([[64.1112]]), None),\n",
       " (tensor([[127.2082]]), None),\n",
       " (tensor([[64.1112]]), None),\n",
       " (tensor([[143.8932]]), None),\n",
       " (tensor([[161.3624]]), None),\n",
       " (tensor([[58.3730]]), None),\n",
       " (tensor([[380.3023]]), None),\n",
       " (tensor([[364.4411]]), None),\n",
       " (tensor([[124.3237]]), None),\n",
       " (tensor([[144.1082]]), None),\n",
       " (tensor([[110.7603]]), None),\n",
       " (tensor([[116.1766]]), None),\n",
       " (tensor([[181.1629]]), None),\n",
       " (tensor([[120.3119]]), None),\n",
       " (tensor([[30.7454]]), None),\n",
       " (tensor([[68.0399]]), None),\n",
       " (tensor([[170.7319]]), None),\n",
       " (tensor([[62.1280]]), None),\n",
       " (tensor([[116.1440]]), None),\n",
       " (tensor([[127.9131]]), None),\n",
       " (tensor([[283.0600]]), None),\n",
       " (tensor([[113.1282]]), None),\n",
       " (tensor([[173.4434]]), None),\n",
       " (tensor([[68.5331]]), None),\n",
       " (tensor([[253.2161]]), None),\n",
       " (tensor([[59.3567]]), None),\n",
       " (tensor([[199.6883]]), None),\n",
       " (tensor([[116.4028]]), None),\n",
       " (tensor([[158.4044]]), None),\n",
       " (tensor([[99.9543]]), None),\n",
       " (tensor([[86.2285]]), None),\n",
       " (tensor([[146.8652]]), None),\n",
       " (tensor([[132.0299]]), None),\n",
       " (tensor([[81.1214]]), None),\n",
       " (tensor([[141.6411]]), None),\n",
       " (tensor([[98.3333]]), None),\n",
       " (tensor([[124.4751]]), None),\n",
       " (tensor([[311.5703]]), None),\n",
       " (tensor([[126.0128]]), None),\n",
       " (tensor([[87.6894]]), None),\n",
       " (tensor([[129.5448]]), None),\n",
       " (tensor([[100.4911]]), None),\n",
       " (tensor([[76.9719]]), None),\n",
       " (tensor([[228.3037]]), None),\n",
       " (tensor([[119.6698]]), None),\n",
       " (tensor([[100.7647]]), None),\n",
       " (tensor([[114.0466]]), None),\n",
       " (tensor([[53.6745]]), None),\n",
       " (tensor([[115.2015]]), None),\n",
       " (tensor([[130.4447]]), None),\n",
       " (tensor([[100.7716]]), None),\n",
       " (tensor([[148.5058]]), None),\n",
       " (tensor([[297.8238]]), None),\n",
       " (tensor([[96.0129]]), None),\n",
       " (tensor([[268.9353]]), None),\n",
       " (tensor([[126.9452]]), None),\n",
       " (tensor([[136.7096]]), None),\n",
       " (tensor([[104.1803]]), None),\n",
       " (tensor([[58.2732]]), None),\n",
       " (tensor([[116.3854]]), None),\n",
       " (tensor([[134.6987]]), None),\n",
       " (tensor([[216.5701]]), None),\n",
       " (tensor([[122.0380]]), None),\n",
       " (tensor([[345.9363]]), None),\n",
       " (tensor([[129.6023]]), None),\n",
       " (tensor([[168.2155]]), None),\n",
       " (tensor([[115.6634]]), None),\n",
       " (tensor([[64.1112]]), None),\n",
       " (tensor([[152.7737]]), None),\n",
       " (tensor([[324.0670]]), None),\n",
       " (tensor([[372.6654]]), None),\n",
       " (tensor([[103.8145]]), None),\n",
       " (tensor([[129.1464]]), None),\n",
       " (tensor([[387.4065]]), None),\n",
       " (tensor([[196.1827]]), None),\n",
       " (tensor([[121.2342]]), None),\n",
       " (tensor([[72.0880]]), None),\n",
       " (tensor([[355.5587]]), None),\n",
       " (tensor([[106.3978]]), None),\n",
       " (tensor([[106.9846]]), None),\n",
       " (tensor([[129.1964]]), None),\n",
       " (tensor([[235.6021]]), None),\n",
       " (tensor([[145.7049]]), None),\n",
       " (tensor([[102.0453]]), None),\n",
       " (tensor([[117.0401]]), None),\n",
       " (tensor([[115.3146]]), None),\n",
       " (tensor([[143.1392]]), None),\n",
       " (tensor([[137.8820]]), None),\n",
       " (tensor([[144.3270]]), None),\n",
       " (tensor([[59.1490]]), None),\n",
       " (tensor([[129.2815]]), None),\n",
       " (tensor([[64.8955]]), None),\n",
       " (tensor([[86.9908]]), None),\n",
       " (tensor([[335.4771]]), None),\n",
       " (tensor([[117.6259]]), None),\n",
       " (tensor([[113.4078]]), None)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(mpnn_chk, pred_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mambaforge-chemprop_gpu]",
   "language": "python",
   "name": "conda-env-mambaforge-chemprop_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
