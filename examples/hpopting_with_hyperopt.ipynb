{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running hyperparameter optimization on Chemprop model using RayTune and Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from lightning import pytorch as pl\n",
    "from ray import tune\n",
    "from ray.train import CheckpointConfig, RunConfig, ScalingConfig\n",
    "from ray.train.lightning import (RayDDPStrategy, RayLightningEnvironment,\n",
    "                                 RayTrainReportCallback, prepare_trainer)\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from chemprop import data, featurizers, models, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemprop_dir = Path.cwd().parent\n",
    "input_path = chemprop_dir / \"tests\" / \"data\" / \"regression\" / \"mol\" / \"mol.csv\" # path to your data .csv file\n",
    "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "smiles_column = 'smiles' # name of the column containing SMILES strings\n",
    "target_columns = ['lipo'] # list of names of the columns containing targets\n",
    "\n",
    "hpopt_save_dir = Path.cwd() / \"hpopt\" # directory to save hyperopt results\n",
    "hpopt_save_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>lipo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cn1c(CN2CCN(CC2)c3ccc(Cl)cc3)nc4ccccc14</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COc1cc(OC)c(cc1NC(=O)CSCC(=O)O)S(=O)(=O)N2C(C)...</td>\n",
       "      <td>-1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COC(=O)[C@@H](N1CCc2sccc2C1)c3ccccc3Cl</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OC[C@H](O)CN1C(=O)C(Cc2ccccc12)NC(=O)c3cc4cc(C...</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cc1cccc(C[C@H](NC(=O)c2cc(nn2C)C(C)(C)C)C(=O)N...</td>\n",
       "      <td>3.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>CC(C)N(CCCNC(=O)Nc1ccc(cc1)C(C)(C)C)C[C@H]2O[C...</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>CCN(CC)CCCCNc1ncc2CN(C(=O)N(Cc3cccc(NC(=O)C=C)...</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>CCSc1c(Cc2ccccc2C(F)(F)F)sc3N(CC(C)C)C(=O)N(C)...</td>\n",
       "      <td>4.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>COc1ccc(Cc2c(N)n[nH]c2N)cc1</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>CCN(CCN(C)C)S(=O)(=O)c1ccc(cc1)c2cnc(N)c(n2)C(...</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               smiles  lipo\n",
       "0             Cn1c(CN2CCN(CC2)c3ccc(Cl)cc3)nc4ccccc14  3.54\n",
       "1   COc1cc(OC)c(cc1NC(=O)CSCC(=O)O)S(=O)(=O)N2C(C)... -1.18\n",
       "2              COC(=O)[C@@H](N1CCc2sccc2C1)c3ccccc3Cl  3.69\n",
       "3   OC[C@H](O)CN1C(=O)C(Cc2ccccc12)NC(=O)c3cc4cc(C...  3.37\n",
       "4   Cc1cccc(C[C@H](NC(=O)c2cc(nn2C)C(C)(C)C)C(=O)N...  3.10\n",
       "..                                                ...   ...\n",
       "95  CC(C)N(CCCNC(=O)Nc1ccc(cc1)C(C)(C)C)C[C@H]2O[C...  2.20\n",
       "96  CCN(CC)CCCCNc1ncc2CN(C(=O)N(Cc3cccc(NC(=O)C=C)...  2.04\n",
       "97  CCSc1c(Cc2ccccc2C(F)(F)F)sc3N(CC(C)C)C(=O)N(C)...  4.49\n",
       "98                        COc1ccc(Cc2c(N)n[nH]c2N)cc1  0.20\n",
       "99  CCN(CCN(C)C)S(=O)(=O)c1ccc(cc1)c2cnc(N)c(n2)C(...  2.00\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input = pd.read_csv(input_path)\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smis = df_input.loc[:, smiles_column].values\n",
    "ys = df_input.loc[:, target_columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make data points, splits, and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smis, ys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = [d.mol for d in all_data]  # RDkit Mol objects are use for structure based splits\n",
    "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"random\", (0.8, 0.1, 0.1))\n",
    "train_data, val_data, test_data = data.split_data_by_indices(\n",
    "    all_data, train_indices, val_indices, test_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "train_dset = data.MoleculeDataset(train_data, featurizer)\n",
    "scaler = train_dset.normalize_targets()\n",
    "\n",
    "val_dset = data.MoleculeDataset(val_data, featurizer)\n",
    "val_dset.normalize_targets(scaler)\n",
    "\n",
    "test_dset = data.MoleculeDataset(test_data, featurizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_dset, val_dset, num_workers, scaler):\n",
    "\n",
    "    # config is a dictionary containing hyperparameters used for the trial\n",
    "    depth = int(config[\"depth\"])\n",
    "    ffn_hidden_dim = int(config[\"ffn_hidden_dim\"])\n",
    "    ffn_num_layers = int(config[\"ffn_num_layers\"])\n",
    "    message_hidden_dim = int(config[\"message_hidden_dim\"])\n",
    "\n",
    "    train_loader = data.build_dataloader(train_dset, num_workers=num_workers, shuffle=True)\n",
    "    val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    mp = nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth)\n",
    "    agg = nn.MeanAggregation()\n",
    "    output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "    ffn = nn.RegressionFFN(output_transform=output_transform, input_dim=message_hidden_dim, hidden_dim=ffn_hidden_dim, n_layers=ffn_num_layers)\n",
    "    batch_norm = True\n",
    "    metric_list = [nn.metrics.RMSEMetric(), nn.metrics.MAEMetric()]\n",
    "    model = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=20, # number of epochs to train for\n",
    "        # below are needed for Ray and Lightning integration\n",
    "        strategy=RayDDPStrategy(find_unused_parameters=True),\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "    )\n",
    "\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"depth\": tune.quniform(lower=2, upper=6, q=1),\n",
    "    \"ffn_hidden_dim\": tune.quniform(lower=300, upper=2400, q=100),\n",
    "    \"ffn_num_layers\": tune.quniform(lower=1, upper=3, q=1),\n",
    "    \"message_hidden_dim\": tune.quniform(lower=300, upper=2400, q=100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-23 12:54:40</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:14.39        </td></tr>\n",
       "<tr><td>Memory:      </td><td>53.1/503.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=2<br>Bracket: Iter 20.000: -0.8531146049499512 | Iter 10.000: -0.934528112411499<br>Logical resource usage: 1.0/64 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  train_loop_config/de\n",
       "pth</th><th style=\"text-align: right;\">     train_loop_config/ff\n",
       "n_hidden_dim</th><th style=\"text-align: right;\">  train_loop_config/ff\n",
       "n_num_layers</th><th style=\"text-align: right;\">    train_loop_config/me\n",
       "ssage_hidden_dim</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  val/rmse</th><th style=\"text-align: right;\">  val/mae</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_4f053456</td><td>TERMINATED</td><td>10.114.0.73:4134154</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">500</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        11.2933 </td><td style=\"text-align: right;\">    0.141701</td><td style=\"text-align: right;\">  0.853115</td><td style=\"text-align: right;\"> 0.676053</td></tr>\n",
       "<tr><td>TorchTrainer_7de7b92f</td><td>TERMINATED</td><td>10.114.0.73:4134251</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">2200</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">400</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         8.39983</td><td style=\"text-align: right;\">    0.350634</td><td style=\"text-align: right;\">  0.936934</td><td style=\"text-align: right;\"> 0.743454</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=4134154)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=4134154)\u001b[0m - (ip=10.114.0.73, pid=4134250) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/pyt ...\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Missing logger folder: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344/artifacts/2024-05-23_12-54-25/TorchTrainer_2024-05-23_12-54-23/working_dirs/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m   | Name            | Type               | Params\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m -------------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m 0 | message_passing | BondMessagePassing | 579 K \n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m 1 | agg             | MeanAggregation    | 0     \n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m 2 | bn              | BatchNorm1d        | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m 3 | predictor       | RegressionFFN      | 9.0 M \n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m 4 | X_d_transform   | Identity           | 0     \n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m -------------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m 9.6 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m 9.6 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m 38.354    Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 39.57it/s, v_num=0, train_loss=1.140]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 558.72it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 33.80it/s, v_num=0, train_loss=1.140, val_loss=0.937]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val/rmse', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val/mae', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m [rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00,  8.80it/s, v_num=0, train_loss=1.140, val_loss=0.937]\n",
      "Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0, train_loss=1.140, val_loss=0.937]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000000)\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-23 12:54:33,342 E 4130654 4130683] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344 is over 95% full, available space: 99949821952; capacity: 2002947665920. Object creation will fail if spilling is required.\n",
      "2024-05-23 12:54:33,694\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 45.38it/s, v_num=0, train_loss=0.887, val_loss=0.937]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 564.13it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 38.17it/s, v_num=0, train_loss=0.887, val_loss=0.942]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  8.95it/s, v_num=0, train_loss=0.887, val_loss=0.942]\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 44.52it/s, v_num=0, train_loss=1.310, val_loss=0.942]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 569.88it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 37.46it/s, v_num=0, train_loss=1.310, val_loss=0.935]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:54:34,060\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  9.22it/s, v_num=0, train_loss=1.310, val_loss=0.935]\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 44.88it/s, v_num=0, train_loss=0.726, val_loss=0.935]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:54:34,384\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 575.51it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 37.69it/s, v_num=0, train_loss=0.726, val_loss=0.921]\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  9.27it/s, v_num=0, train_loss=0.726, val_loss=0.921]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0, train_loss=0.726, val_loss=0.921]        \n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 40.28it/s, v_num=0, train_loss=0.387, val_loss=0.921]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 574.64it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 34.55it/s, v_num=0, train_loss=0.387, val_loss=0.932]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:54:34,713\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00,  8.93it/s, v_num=0, train_loss=0.387, val_loss=0.932]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:54:35,063\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 42.23it/s, v_num=0, train_loss=0.604, val_loss=0.932]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 564.59it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 35.97it/s, v_num=0, train_loss=0.604, val_loss=0.960]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000005)\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m 2 | bn              | BatchNorm1d        | 800   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00,  9.26it/s, v_num=0, train_loss=0.604, val_loss=0.960]\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 41.30it/s, v_num=0, train_loss=0.572, val_loss=0.960]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 561.71it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 35.21it/s, v_num=0, train_loss=0.572, val_loss=0.961]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:54:35,403\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000006)\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00,  8.99it/s, v_num=0, train_loss=0.572, val_loss=0.961]\n",
      "Epoch 7:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0, train_loss=0.572, val_loss=0.961]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000007)\n",
      "\u001b[36m(TorchTrainer pid=4134251)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=4134251)\u001b[0m - (ip=10.114.0.73, pid=4134486) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "2024-05-23 12:54:35,748\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 42.28it/s, v_num=0, train_loss=0.164, val_loss=0.961]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 577.01it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 35.93it/s, v_num=0, train_loss=0.164, val_loss=0.939]\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00,  8.89it/s, v_num=0, train_loss=0.164, val_loss=0.939]\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 42.02it/s, v_num=0, train_loss=0.255, val_loss=0.939]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 565.73it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 35.77it/s, v_num=0, train_loss=0.255, val_loss=0.944]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:54:36,108\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:36,113\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00,  8.39it/s, v_num=0, train_loss=0.255, val_loss=0.944]\n",
      "Epoch 9:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0, train_loss=0.255, val_loss=0.944]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:54:36,483\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:36,508\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:36,856\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2/2 [00:00<00:00, 42.36it/s, v_num=0, train_loss=0.274, val_loss=0.932]\n",
      "Epoch 2:  50%|█████     | 1/2 [00:00<00:00, 29.09it/s, v_num=0, train_loss=0.786, val_loss=0.937]\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 42.46it/s, v_num=0, train_loss=1.440, val_loss=0.937]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/pyt ...\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m Missing logger folder: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344/artifacts/2024-05-23_12-54-25/TorchTrainer_2024-05-23_12-54-23/working_dirs/TorchTrainer_7de7b92f_2_depth=2.0000,ffn_hidden_dim=2200.0000,ffn_num_layers=3.0000,message_hidden_dim=400.0000_2024-05-23_12-54-28/lightning_logs\n",
      "2024-05-23 12:54:36,880\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:37,194\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:37,308\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:37,554\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m   | Name            | Type               | Params\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m -------------------------------------------------------\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m 0 | message_passing | BondMessagePassing | 383 K \n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m 1 | agg             | MeanAggregation    | 0     \n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m 3 | predictor       | RegressionFFN      | 10.6 M\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m 4 | X_d_transform   | Identity           | 0     \n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m 11.0 M    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m 11.0 M    Total params\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m 43.813    Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "2024-05-23 12:54:37,695\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:54:37,905\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:38,077\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:38,287\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val/rmse', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val/mae', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m [rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "2024-05-23 12:54:38,485\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n",
      "Epoch 13: 100%|██████████| 2/2 [00:00<00:00, 41.12it/s, v_num=0, train_loss=0.0767, val_loss=0.885]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 580.53it/s]\u001b[A\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 14: 100%|██████████| 2/2 [00:00<00:00, 36.61it/s, v_num=0, train_loss=0.072, val_loss=0.889]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 14: 100%|██████████| 2/2 [00:00<00:00,  8.51it/s, v_num=0, train_loss=0.072, val_loss=0.889]\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_7de7b92f_2_depth=2.0000,ffn_hidden_dim=2200.0000,ffn_num_layers=3.0000,message_hidden_dim=400.0000_2024-05-23_12-54-28/checkpoint_000006)\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "2024-05-23 12:54:38,667\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0, train_loss=0.072, val_loss=0.889]\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:54:38,894\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:39,023\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 2/2 [00:00<00:00, 41.18it/s, v_num=0, train_loss=0.0818, val_loss=0.891]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:54:39,290\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:39,401\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=4134486)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:54:39,697\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:39,762\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:40,149\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:40,152\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-23 12:54:40,153\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23' in 0.0029s.\n",
      "2024-05-23 12:54:40,156\tINFO tune.py:1048 -- Total run time: 14.40 seconds (14.39 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=4134250)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000019)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  8.43it/s, v_num=0, train_loss=0.142, val_loss=0.853]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-05-23 12:54:43,345 E 4130654 4130683] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344 is over 95% full, available space: 99539296256; capacity: 2002947665920. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-23 12:54:53,349 E 4130654 4130683] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344 is over 95% full, available space: 99539243008; capacity: 2002947665920. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-23 12:55:03,352 E 4130654 4130683] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344 is over 95% full, available space: 99539222528; capacity: 2002947665920. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-23 12:55:13,356 E 4130654 4130683] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344 is over 95% full, available space: 99539210240; capacity: 2002947665920. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-23 12:55:23,359 E 4130654 4130683] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344 is over 95% full, available space: 99539165184; capacity: 2002947665920. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-23 12:55:33,363 E 4130654 4130683] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344 is over 95% full, available space: 99538870272; capacity: 2002947665920. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-23 12:55:43,367 E 4130654 4130683] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344 is over 95% full, available space: 99538853888; capacity: 2002947665920. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-23 12:55:53,372 E 4130654 4130683] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344 is over 95% full, available space: 99538997248; capacity: 2002947665920. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-05-23 12:56:03,376 E 4130654 4130683] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-05-23_12-54-23_673652_4130344 is over 95% full, available space: 99538984960; capacity: 2002947665920. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "    max_t=20,\n",
    "    grace_period=10,\n",
    "    reduction_factor=2,\n",
    ")\n",
    "\n",
    "# Scaling config controls the resources used by Ray\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=1,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "# Checkpoint config controls the checkpointing behavior of Ray\n",
    "checkpoint_config = CheckpointConfig(\n",
    "    num_to_keep=1, # number of checkpoints to keep\n",
    "    checkpoint_score_attribute=\"val_loss\", # Save the checkpoint based on this metric\n",
    "    checkpoint_score_order=\"min\", # Save the checkpoint with the lowest metric value\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=checkpoint_config,\n",
    "    storage_path=hpopt_save_dir / \"ray_results\", # directory to save the results\n",
    ")\n",
    "\n",
    "ray_trainer = TorchTrainer(\n",
    "    lambda config: train_model(\n",
    "        config, train_dset, val_dset, num_workers, scaler\n",
    "    ),\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "search_alg = HyperOptSearch(\n",
    "    n_initial_points=1, # number of random evaluations before tree parzen estimators\n",
    "    random_state_seed=42,\n",
    ")\n",
    "\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    num_samples=2, # number of trials to run\n",
    "    scheduler=scheduler,\n",
    "    search_alg=search_alg,\n",
    "    \n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    ray_trainer,\n",
    "    param_space={\n",
    "        \"train_loop_config\": search_space,\n",
    "    },\n",
    "    tune_config=tune_config,\n",
    ")\n",
    "\n",
    "# Start the hyperparameter search\n",
    "results = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    metrics={'train_loss': 0.14170069992542267, 'val/rmse': 0.8531146049499512, 'val/mae': 0.6760528683662415, 'val_loss': 0.8531146049499512, 'epoch': 19, 'step': 40},\n",
       "    path='/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000019)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'train_loss': 0.3506341576576233, 'val/rmse': 0.9369344711303711, 'val/mae': 0.7434543967247009, 'val_loss': 0.9369344711303711, 'epoch': 9, 'step': 20},\n",
       "    path='/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_7de7b92f_2_depth=2.0000,ffn_hidden_dim=2200.0000,ffn_num_layers=3.0000,message_hidden_dim=400.0000_2024-05-23_12-54-28',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_7de7b92f_2_depth=2.0000,ffn_hidden_dim=2200.0000,ffn_num_layers=3.0000,message_hidden_dim=400.0000_2024-05-23_12-54-28/checkpoint_000009)\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val/rmse</th>\n",
       "      <th>val/mae</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>checkpoint_dir_name</th>\n",
       "      <th>should_checkpoint</th>\n",
       "      <th>done</th>\n",
       "      <th>...</th>\n",
       "      <th>pid</th>\n",
       "      <th>hostname</th>\n",
       "      <th>node_ip</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>config/train_loop_config/depth</th>\n",
       "      <th>config/train_loop_config/ffn_hidden_dim</th>\n",
       "      <th>config/train_loop_config/ffn_num_layers</th>\n",
       "      <th>config/train_loop_config/message_hidden_dim</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.141701</td>\n",
       "      <td>0.853115</td>\n",
       "      <td>0.676053</td>\n",
       "      <td>0.853115</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "      <td>1716483280</td>\n",
       "      <td>checkpoint_000019</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>4134154</td>\n",
       "      <td>estes</td>\n",
       "      <td>10.114.0.73</td>\n",
       "      <td>11.29332</td>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>4f053456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350634</td>\n",
       "      <td>0.936934</td>\n",
       "      <td>0.743454</td>\n",
       "      <td>0.936934</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>1716483279</td>\n",
       "      <td>checkpoint_000009</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>4134251</td>\n",
       "      <td>estes</td>\n",
       "      <td>10.114.0.73</td>\n",
       "      <td>8.39983</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>7de7b92f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss  val/rmse   val/mae  val_loss  epoch  step   timestamp  \\\n",
       "0    0.141701  0.853115  0.676053  0.853115     19    40  1716483280   \n",
       "1    0.350634  0.936934  0.743454  0.936934      9    20  1716483279   \n",
       "\n",
       "  checkpoint_dir_name  should_checkpoint  done  ...      pid hostname  \\\n",
       "0   checkpoint_000019               True  True  ...  4134154    estes   \n",
       "1   checkpoint_000009               True  True  ...  4134251    estes   \n",
       "\n",
       "       node_ip  time_since_restore  iterations_since_restore  \\\n",
       "0  10.114.0.73            11.29332                        20   \n",
       "1  10.114.0.73             8.39983                        10   \n",
       "\n",
       "   config/train_loop_config/depth config/train_loop_config/ffn_hidden_dim  \\\n",
       "0                             2.0                                  2000.0   \n",
       "1                             2.0                                  2200.0   \n",
       "\n",
       "  config/train_loop_config/ffn_num_layers  \\\n",
       "0                                     3.0   \n",
       "1                                     3.0   \n",
       "\n",
       "   config/train_loop_config/message_hidden_dim    logdir  \n",
       "0                                        500.0  4f053456  \n",
       "1                                        400.0  7de7b92f  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results of all trials\n",
    "result_df = results.get_dataframe()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth': 2.0,\n",
       " 'ffn_hidden_dim': 2000.0,\n",
       " 'ffn_num_layers': 3.0,\n",
       " 'message_hidden_dim': 500.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best configuration\n",
    "best_result = results.get_best_result()\n",
    "best_config = best_result.config\n",
    "best_config['train_loop_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model checkpoint path: /home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-05-23_12-54-23/TorchTrainer_4f053456_1_depth=2.0000,ffn_hidden_dim=2000.0000,ffn_num_layers=3.0000,message_hidden_dim=500.0000_2024-05-23_12-54-25/checkpoint_000019/checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "# best model checkpoint path\n",
    "best_result = results.get_best_result()\n",
    "best_checkpoint_path = Path(best_result.checkpoint.path) / \"checkpoint.ckpt\"\n",
    "print(f\"Best model checkpoint path: {best_checkpoint_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop_v2_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
