{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running hyperparameter optimization on Chemprop model using RayTune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from lightning import pytorch as pl\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.train import CheckpointConfig, RunConfig, ScalingConfig\n",
    "from ray.train.lightning import (RayDDPStrategy, RayLightningEnvironment,\n",
    "                                 RayTrainReportCallback, prepare_trainer)\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import FIFOScheduler\n",
    "\n",
    "from chemprop import data, featurizers, models, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemprop_dir = Path.cwd().parent\n",
    "input_path = chemprop_dir / \"tests\" / \"data\" / \"regression\" / \"mol\" / \"mol.csv\" # path to your data .csv file\n",
    "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "smiles_column = 'smiles' # name of the column containing SMILES strings\n",
    "target_columns = ['lipo'] # list of names of the columns containing targets\n",
    "\n",
    "hpopt_save_dir = Path.cwd() / \"hpopt\" # directory to save hyperopt results\n",
    "hpopt_save_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>lipo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cn1c(CN2CCN(CC2)c3ccc(Cl)cc3)nc4ccccc14</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COc1cc(OC)c(cc1NC(=O)CSCC(=O)O)S(=O)(=O)N2C(C)...</td>\n",
       "      <td>-1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COC(=O)[C@@H](N1CCc2sccc2C1)c3ccccc3Cl</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OC[C@H](O)CN1C(=O)C(Cc2ccccc12)NC(=O)c3cc4cc(C...</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cc1cccc(C[C@H](NC(=O)c2cc(nn2C)C(C)(C)C)C(=O)N...</td>\n",
       "      <td>3.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>CC(C)N(CCCNC(=O)Nc1ccc(cc1)C(C)(C)C)C[C@H]2O[C...</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>CCN(CC)CCCCNc1ncc2CN(C(=O)N(Cc3cccc(NC(=O)C=C)...</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>CCSc1c(Cc2ccccc2C(F)(F)F)sc3N(CC(C)C)C(=O)N(C)...</td>\n",
       "      <td>4.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>COc1ccc(Cc2c(N)n[nH]c2N)cc1</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>CCN(CCN(C)C)S(=O)(=O)c1ccc(cc1)c2cnc(N)c(n2)C(...</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               smiles  lipo\n",
       "0             Cn1c(CN2CCN(CC2)c3ccc(Cl)cc3)nc4ccccc14  3.54\n",
       "1   COc1cc(OC)c(cc1NC(=O)CSCC(=O)O)S(=O)(=O)N2C(C)... -1.18\n",
       "2              COC(=O)[C@@H](N1CCc2sccc2C1)c3ccccc3Cl  3.69\n",
       "3   OC[C@H](O)CN1C(=O)C(Cc2ccccc12)NC(=O)c3cc4cc(C...  3.37\n",
       "4   Cc1cccc(C[C@H](NC(=O)c2cc(nn2C)C(C)(C)C)C(=O)N...  3.10\n",
       "..                                                ...   ...\n",
       "95  CC(C)N(CCCNC(=O)Nc1ccc(cc1)C(C)(C)C)C[C@H]2O[C...  2.20\n",
       "96  CCN(CC)CCCCNc1ncc2CN(C(=O)N(Cc3cccc(NC(=O)C=C)...  2.04\n",
       "97  CCSc1c(Cc2ccccc2C(F)(F)F)sc3N(CC(C)C)C(=O)N(C)...  4.49\n",
       "98                        COc1ccc(Cc2c(N)n[nH]c2N)cc1  0.20\n",
       "99  CCN(CCN(C)C)S(=O)(=O)c1ccc(cc1)c2cnc(N)c(n2)C(...  2.00\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input = pd.read_csv(input_path)\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smis = df_input.loc[:, smiles_column].values\n",
    "ys = df_input.loc[:, target_columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make data points, splits, and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smis, ys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = [d.mol for d in all_data]  # RDkit Mol objects are use for structure based splits\n",
    "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"random\", (0.8, 0.1, 0.1))\n",
    "train_data, val_data, test_data = data.split_data_by_indices(\n",
    "    all_data, train_indices, val_indices, test_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "train_dset = data.MoleculeDataset(train_data, featurizer)\n",
    "scaler = train_dset.normalize_targets()\n",
    "\n",
    "val_dset = data.MoleculeDataset(val_data, featurizer)\n",
    "val_dset.normalize_targets(scaler)\n",
    "\n",
    "test_dset = data.MoleculeDataset(test_data, featurizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_dset, val_dset, num_workers, scaler):\n",
    "\n",
    "    # config is a dictionary containing hyperparameters used for the trial\n",
    "    depth = int(config[\"depth\"])\n",
    "    ffn_hidden_dim = int(config[\"ffn_hidden_dim\"])\n",
    "    ffn_num_layers = int(config[\"ffn_num_layers\"])\n",
    "    message_hidden_dim = int(config[\"message_hidden_dim\"])\n",
    "\n",
    "    train_loader = data.build_dataloader(train_dset, num_workers=num_workers, shuffle=True)\n",
    "    val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    mp = nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth)\n",
    "    agg = nn.MeanAggregation()\n",
    "    output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "    ffn = nn.RegressionFFN(output_transform=output_transform, input_dim=message_hidden_dim, hidden_dim=ffn_hidden_dim, n_layers=ffn_num_layers)\n",
    "    batch_norm = True\n",
    "    metric_list = [nn.metrics.RMSEMetric(), nn.metrics.MAEMetric()]\n",
    "    model = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=20, # number of epochs to train for\n",
    "        # below are needed for Ray and Lightning integration\n",
    "        strategy=RayDDPStrategy(find_unused_parameters=True),\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "    )\n",
    "\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"depth\": tune.qrandint(lower=2, upper=6, q=1),\n",
    "    \"ffn_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
    "    \"ffn_num_layers\": tune.qrandint(lower=1, upper=3, q=1),\n",
    "    \"message_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-06-05 11:25:33</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:18.91        </td></tr>\n",
       "<tr><td>Memory:      </td><td>67.5/503.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/64 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                                                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_c8cd4ba9</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-06-05_11-25-12_802597_14540/artifacts/2024-06-05_11-25-14/TorchTrainer_2024-06-05_11-25-14/driver_artifacts/TorchTrainer_c8cd4ba9_1_depth=2,ffn_hidden_dim=2000,ffn_num_layers=2,message_hidden_dim=500_2024-06-05_11-25-14/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">  train_loop_config/de\n",
       "pth</th><th style=\"text-align: right;\">     train_loop_config/ff\n",
       "n_hidden_dim</th><th style=\"text-align: right;\">  train_loop_config/ff\n",
       "n_num_layers</th><th style=\"text-align: right;\">    train_loop_config/me\n",
       "ssage_hidden_dim</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  val/rmse</th><th style=\"text-align: right;\">  val/mae</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_bdfa8a4b</td><td>TERMINATED</td><td>10.114.0.73:18449</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">2200</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">400</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         12.1131</td><td style=\"text-align: right;\">    0.163928</td><td style=\"text-align: right;\">  0.868934</td><td style=\"text-align: right;\"> 0.706374</td></tr>\n",
       "<tr><td>TorchTrainer_c8cd4ba9</td><td>ERROR     </td><td>10.114.0.73:18351</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">500</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">            </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">         </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=18351)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=18351)\u001b[0m - (ip=10.114.0.73, pid=18448) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/pyt ...\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m Missing logger folder: /tmp/ray/session_2024-06-05_11-25-12_802597_14540/artifacts/2024-06-05_11-25-14/TorchTrainer_2024-06-05_11-25-14/working_dirs/TorchTrainer_c8cd4ba9_1_depth=2,ffn_hidden_dim=2000,ffn_num_layers=2,message_hidden_dim=500_2024-06-05_11-25-14/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m   | Name            | Type               | Params\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m -------------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m 0 | message_passing | BondMessagePassing | 579 K \n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m 1 | agg             | MeanAggregation    | 0     \n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m 2 | bn              | BatchNorm1d        | 1.0 K \n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m 3 | predictor       | RegressionFFN      | 5.0 M \n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m 4 | X_d_transform   | Identity           | 0     \n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m -------------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m 5.6 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m 5.6 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m 22.346    Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-06-05 11:25:24,245 E 14850 14879] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-06-05_11-25-12_802597_14540 is over 95% full, available space: 99562876928; capacity: 2002947665920. Object creation will fail if spilling is required.\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val/rmse', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val/mae', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=18448)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "2024-06-05 11:25:24,559\tERROR tune_controller.py:1332 -- Trial task failed for trial TorchTrainer_c8cd4ba9\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::_Inner.train()\u001b[39m (pid=18351, ip=10.114.0.73, actor_id=60687203e2edded2c00f26df01000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 334, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 53, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=18448, ip=10.114.0.73, actor_id=fbccd7bc76945e426f5f8ad201000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7ee48a5ae4d0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/ray/train/_internal/utils.py\", line 169, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_14540/4292595963.py\", line 24, in <lambda>\n",
      "  File \"/tmp/ipykernel_14540/3664663599.py\", line 31, in train_model\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 987, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1033, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 250, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 190, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 268, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1303, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 152, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/strategies/ddp.py\", line 270, in optimizer_step\n",
      "    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 122, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/optim/adam.py\", line 146, in step\n",
      "    loss = closure()\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 108, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 144, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 129, in closure\n",
      "    step_output = self._step_fn()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 318, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 390, in training_step\n",
      "    return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 642, in __call__\n",
      "    wrapper_output = wrapper_module(*args, **kwargs)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n",
      "    else self._run_ddp_forward(*inputs, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n",
      "    return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 635, in wrapped_forward\n",
      "    out = method(*_args, **_kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/Projects/chemprop_v2_dev/chemprop/chemprop/models/model.py\", line 144, in training_step\n",
      "    Z = self.fingerprint(bmg, V_d, X_d)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/Projects/chemprop_v2_dev/chemprop/chemprop/models/model.py\", line 120, in fingerprint\n",
      "    H_v = self.message_passing(bmg, V_d)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/Projects/chemprop_v2_dev/chemprop/chemprop/nn/message_passing/base.py\", line 205, in forward\n",
      "    M = self.message(H, bmg)\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hwpang/Projects/chemprop_v2_dev/chemprop/chemprop/nn/message_passing/base.py\", line 259, in message\n",
      "    M_all = torch.zeros(len(bmg.V), H.shape[1], dtype=H.dtype, device=H.device).scatter_reduce_(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 25.06 MiB is free. Process 3266 has 390.70 MiB memory in use. Process 1723282 has 1.55 GiB memory in use. Process 4176779 has 20.07 GiB memory in use. Including non-PyTorch memory, this process has 1.53 GiB memory in use. Of the allocated memory 89.29 MiB is allocated by PyTorch, and 8.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=18449)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=18449)\u001b[0m - (ip=10.114.0.73, pid=18682) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m 2 | bn              | BatchNorm1d        | 800   \n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/pyt ...\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Missing logger folder: /tmp/ray/session_2024-06-05_11-25-12_802597_14540/artifacts/2024-06-05_11-25-14/TorchTrainer_2024-06-05_11-25-14/working_dirs/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m [rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "2024-06-05 11:25:27,342\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 29.43it/s, v_num=0, train_loss=1.410]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 135.95it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 21.88it/s, v_num=0, train_loss=1.410, val_loss=0.941]\n",
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00,  9.84it/s, v_num=0, train_loss=1.410, val_loss=0.941]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000000)\n",
      "2024-06-05 11:25:27,618\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00, 25.14it/s, v_num=0, train_loss=1.410, val_loss=0.941]\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 29.97it/s, v_num=0, train_loss=0.369, val_loss=0.941]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 135.95it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 22.20it/s, v_num=0, train_loss=0.369, val_loss=0.937]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000001)\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m   | Name            | Type               | Params\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m -------------------------------------------------------\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m 0 | message_passing | BondMessagePassing | 383 K \n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m 1 | agg             | MeanAggregation    | 0     \n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m 3 | predictor       | RegressionFFN      | 5.7 M \n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m 4 | X_d_transform   | Identity           | 0     \n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m 6.1 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m 6.1 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m 24.444    Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 10.39it/s, v_num=0, train_loss=0.369, val_loss=0.937]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 32.63it/s, v_num=0, train_loss=1.200, val_loss=0.937]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 557.90it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 28.80it/s, v_num=0, train_loss=1.200, val_loss=0.924]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000002)\n",
      "2024-06-05 11:25:27,874\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 11.12it/s, v_num=0, train_loss=1.200, val_loss=0.924]\n",
      "Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0, train_loss=1.200, val_loss=0.924]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 11:25:28,146\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 29.38it/s, v_num=0, train_loss=0.731, val_loss=0.924]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 246.85it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 23.13it/s, v_num=0, train_loss=0.731, val_loss=0.940]\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 10.36it/s, v_num=0, train_loss=0.731, val_loss=0.940]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000003)\n",
      "2024-06-05 11:25:28,432\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  50%|█████     | 1/2 [00:00<00:00, 23.05it/s, v_num=0, train_loss=0.715, val_loss=0.940]\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 30.10it/s, v_num=0, train_loss=0.592, val_loss=0.940]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 136.06it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 22.29it/s, v_num=0, train_loss=0.592, val_loss=0.949]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 10.20it/s, v_num=0, train_loss=0.592, val_loss=0.949]\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 28.70it/s, v_num=0, train_loss=0.627, val_loss=0.949]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 476.95it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 25.19it/s, v_num=0, train_loss=0.627, val_loss=0.942]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 11:25:28,694\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 11.02it/s, v_num=0, train_loss=0.627, val_loss=0.942]\n",
      "Epoch 6:  50%|█████     | 1/2 [00:00<00:00, 29.28it/s, v_num=0, train_loss=0.440, val_loss=0.942]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 11:25:28,948\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 39.85it/s, v_num=0, train_loss=0.456, val_loss=0.942]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 135.99it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 27.22it/s, v_num=0, train_loss=0.456, val_loss=0.936]\n",
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 11.37it/s, v_num=0, train_loss=0.456, val_loss=0.936]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000006)\n",
      "2024-06-05 11:25:29,214\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 29.63it/s, v_num=0, train_loss=0.421, val_loss=0.936]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 239.92it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 23.51it/s, v_num=0, train_loss=0.421, val_loss=0.937]\n",
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 10.62it/s, v_num=0, train_loss=0.421, val_loss=0.937]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000007)\n",
      "2024-06-05 11:25:29,487\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0, train_loss=0.421, val_loss=0.937]        \n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 28.87it/s, v_num=0, train_loss=0.377, val_loss=0.937]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 136.22it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 21.61it/s, v_num=0, train_loss=0.377, val_loss=0.946]\n",
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000008)\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val/rmse', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val/mae', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m /home/hwpang/miniforge3/envs/chemprop_v2_dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 10.26it/s, v_num=0, train_loss=0.377, val_loss=0.946]\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 30.42it/s, v_num=0, train_loss=0.405, val_loss=0.946]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 11:25:29,759\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 135.95it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 22.77it/s, v_num=0, train_loss=0.405, val_loss=0.946]\n",
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 10.25it/s, v_num=0, train_loss=0.405, val_loss=0.946]\n",
      "Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0, train_loss=0.405, val_loss=0.946]       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000009)\n",
      "2024-06-05 11:25:30,021\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2/2 [00:00<00:00, 31.33it/s, v_num=0, train_loss=0.204, val_loss=0.946]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 565.42it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 2/2 [00:00<00:00, 25.05it/s, v_num=0, train_loss=0.204, val_loss=0.936]\n",
      "Epoch 10: 100%|██████████| 2/2 [00:00<00:00, 11.04it/s, v_num=0, train_loss=0.204, val_loss=0.936]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000010)\n",
      "2024-06-05 11:25:30,286\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  50%|█████     | 1/2 [00:00<00:00, 25.64it/s, v_num=0, train_loss=0.186, val_loss=0.936]\n",
      "Epoch 11: 100%|██████████| 2/2 [00:00<00:00, 32.91it/s, v_num=0, train_loss=0.260, val_loss=0.936]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 136.18it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 2/2 [00:00<00:00, 23.81it/s, v_num=0, train_loss=0.260, val_loss=0.926]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 2/2 [00:00<00:00, 10.69it/s, v_num=0, train_loss=0.260, val_loss=0.926]\n",
      "Epoch 12: 100%|██████████| 2/2 [00:00<00:00, 34.54it/s, v_num=0, train_loss=0.136, val_loss=0.926]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 135.68it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 2/2 [00:00<00:00, 24.64it/s, v_num=0, train_loss=0.136, val_loss=0.920]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000012)\n",
      "2024-06-05 11:25:30,563\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 2/2 [00:00<00:00, 10.37it/s, v_num=0, train_loss=0.136, val_loss=0.920]\n",
      "Epoch 13:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0, train_loss=0.136, val_loss=0.920]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 11:25:30,835\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 2/2 [00:00<00:00, 33.93it/s, v_num=0, train_loss=0.104, val_loss=0.920]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 136.30it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 2/2 [00:00<00:00, 24.34it/s, v_num=0, train_loss=0.104, val_loss=0.914]\n",
      "Epoch 13: 100%|██████████| 2/2 [00:00<00:00, 10.32it/s, v_num=0, train_loss=0.104, val_loss=0.914]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000013)\n",
      "2024-06-05 11:25:31,097\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 2/2 [00:00<00:00, 37.08it/s, v_num=0, train_loss=0.121, val_loss=0.914]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 135.46it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 2/2 [00:00<00:00, 25.71it/s, v_num=0, train_loss=0.121, val_loss=0.907]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000014)\n",
      "2024-06-05 11:25:31,371\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0, train_loss=0.121, val_loss=0.907]        \n",
      "Epoch 15: 100%|██████████| 2/2 [00:00<00:00, 31.46it/s, v_num=0, train_loss=0.105, val_loss=0.907]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 135.31it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 2/2 [00:00<00:00, 23.03it/s, v_num=0, train_loss=0.105, val_loss=0.902]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000015)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 2/2 [00:00<00:00, 10.53it/s, v_num=0, train_loss=0.105, val_loss=0.902]\n",
      "Epoch 16:  50%|█████     | 1/2 [00:00<00:00, 25.69it/s, v_num=0, train_loss=0.105, val_loss=0.902]\n",
      "Epoch 16:  50%|█████     | 1/2 [00:00<00:00, 24.50it/s, v_num=0, train_loss=0.100, val_loss=0.902]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 11:25:31,633\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 2/2 [00:00<00:00, 31.23it/s, v_num=0, train_loss=0.0775, val_loss=0.902]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 554.51it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 2/2 [00:00<00:00, 26.22it/s, v_num=0, train_loss=0.0775, val_loss=0.897]\n",
      "Epoch 16: 100%|██████████| 2/2 [00:00<00:00, 11.18it/s, v_num=0, train_loss=0.0775, val_loss=0.897]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000016)\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000017)\n",
      "2024-06-05 11:25:31,917\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 2/2 [00:00<00:00, 31.18it/s, v_num=0, train_loss=0.119, val_loss=0.897] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 135.71it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m \n",
      "Epoch 17: 100%|██████████| 2/2 [00:00<00:00, 22.89it/s, v_num=0, train_loss=0.119, val_loss=0.889]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 11:25:32,187\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0, train_loss=0.119, val_loss=0.889]        \n",
      "Epoch 18: 100%|██████████| 2/2 [00:00<00:00, 30.74it/s, v_num=0, train_loss=0.0772, val_loss=0.889]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 591.91it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 2/2 [00:00<00:00, 26.70it/s, v_num=0, train_loss=0.0772, val_loss=0.880]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000018)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 2/2 [00:00<00:00, 11.02it/s, v_num=0, train_loss=0.0772, val_loss=0.880]\n",
      "Epoch 19:  50%|█████     | 1/2 [00:00<00:00, 25.39it/s, v_num=0, train_loss=0.0684, val_loss=0.880]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 11:25:32,471\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00, 32.03it/s, v_num=0, train_loss=0.164, val_loss=0.880] \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 135.80it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00, 23.36it/s, v_num=0, train_loss=0.164, val_loss=0.869]\n",
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00, 10.31it/s, v_num=0, train_loss=0.164, val_loss=0.869]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000019)\n",
      "\u001b[36m(RayTrainWorker pid=18682)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  7.20it/s, v_num=0, train_loss=0.164, val_loss=0.869]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 11:25:33,705\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-06-05 11:25:33,706\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14' in 0.0027s.\n",
      "2024-06-05 11:25:33,709\tERROR tune.py:1044 -- Trials did not complete: [TorchTrainer_c8cd4ba9]\n",
      "2024-06-05 11:25:33,709\tINFO tune.py:1048 -- Total run time: 18.92 seconds (18.90 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "\n",
    "scheduler = FIFOScheduler()\n",
    "\n",
    "# Scaling config controls the resources used by Ray\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=1,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "# Checkpoint config controls the checkpointing behavior of Ray\n",
    "checkpoint_config = CheckpointConfig(\n",
    "    num_to_keep=1, # number of checkpoints to keep\n",
    "    checkpoint_score_attribute=\"val_loss\", # Save the checkpoint based on this metric\n",
    "    checkpoint_score_order=\"min\", # Save the checkpoint with the lowest metric value\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=checkpoint_config,\n",
    "    storage_path=hpopt_save_dir / \"ray_results\", # directory to save the results\n",
    ")\n",
    "\n",
    "ray_trainer = TorchTrainer(\n",
    "    lambda config: train_model(\n",
    "        config, train_dset, val_dset, num_workers, scaler\n",
    "    ),\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "search_alg = HyperOptSearch(\n",
    "    n_initial_points=1, # number of random evaluations before tree parzen estimators\n",
    "    random_state_seed=42,\n",
    ")\n",
    "\n",
    "# OptunaSearch is another search algorithm that can be used\n",
    "# search_alg = OptunaSearch() \n",
    "\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    num_samples=2, # number of trials to run\n",
    "    scheduler=scheduler,\n",
    "    search_alg=search_alg,\n",
    "    \n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    ray_trainer,\n",
    "    param_space={\n",
    "        \"train_loop_config\": search_space,\n",
    "    },\n",
    "    tune_config=tune_config,\n",
    ")\n",
    "\n",
    "# Start the hyperparameter search\n",
    "results = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    error='RayTaskError(OutOfMemoryError)',\n",
       "    metrics={},\n",
       "    path='/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_c8cd4ba9_1_depth=2,ffn_hidden_dim=2000,ffn_num_layers=2,message_hidden_dim=500_2024-06-05_11-25-14',\n",
       "    filesystem='local',\n",
       "    checkpoint=None\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'train_loss': 0.16392788290977478, 'val/rmse': 0.8689340353012085, 'val/mae': 0.7063738107681274, 'val_loss': 0.8689340353012085, 'epoch': 19, 'step': 40},\n",
       "    path='/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000019)\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val/rmse</th>\n",
       "      <th>val/mae</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>checkpoint_dir_name</th>\n",
       "      <th>should_checkpoint</th>\n",
       "      <th>done</th>\n",
       "      <th>...</th>\n",
       "      <th>pid</th>\n",
       "      <th>hostname</th>\n",
       "      <th>node_ip</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>config/train_loop_config/depth</th>\n",
       "      <th>config/train_loop_config/ffn_hidden_dim</th>\n",
       "      <th>config/train_loop_config/ffn_num_layers</th>\n",
       "      <th>config/train_loop_config/message_hidden_dim</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.163928</td>\n",
       "      <td>0.868934</td>\n",
       "      <td>0.706374</td>\n",
       "      <td>0.868934</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "      <td>1717601132</td>\n",
       "      <td>checkpoint_000019</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>18449</td>\n",
       "      <td>estes</td>\n",
       "      <td>10.114.0.73</td>\n",
       "      <td>12.113055</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2200</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>bdfa8a4b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss  val/rmse   val/mae  val_loss  epoch  step   timestamp  \\\n",
       "0    0.163928  0.868934  0.706374  0.868934     19    40  1717601132   \n",
       "\n",
       "  checkpoint_dir_name  should_checkpoint   done  ...    pid hostname  \\\n",
       "0   checkpoint_000019               True  False  ...  18449    estes   \n",
       "\n",
       "       node_ip  time_since_restore  iterations_since_restore  \\\n",
       "0  10.114.0.73           12.113055                        20   \n",
       "\n",
       "   config/train_loop_config/depth config/train_loop_config/ffn_hidden_dim  \\\n",
       "0                               2                                    2200   \n",
       "\n",
       "  config/train_loop_config/ffn_num_layers  \\\n",
       "0                                       2   \n",
       "\n",
       "   config/train_loop_config/message_hidden_dim    logdir  \n",
       "0                                          400  bdfa8a4b  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results of all trials\n",
    "result_df = results.get_dataframe()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth': 2,\n",
       " 'ffn_hidden_dim': 2200,\n",
       " 'ffn_num_layers': 2,\n",
       " 'message_hidden_dim': 400}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best configuration\n",
    "best_result = results.get_best_result()\n",
    "best_config = best_result.config\n",
    "best_config['train_loop_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model checkpoint path: /home/hwpang/Projects/chemprop_v2_dev/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-06-05_11-25-14/TorchTrainer_bdfa8a4b_2_depth=2,ffn_hidden_dim=2200,ffn_num_layers=2,message_hidden_dim=400_2024-06-05_11-25-17/checkpoint_000019/checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "# best model checkpoint path\n",
    "best_result = results.get_best_result()\n",
    "best_checkpoint_path = Path(best_result.checkpoint.path) / \"checkpoint.ckpt\"\n",
    "print(f\"Best model checkpoint path: {best_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop_v2_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
