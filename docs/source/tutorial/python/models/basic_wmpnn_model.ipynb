{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chemprop wMPNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemprop.models import wMPNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a Chemprop [MPNN](./basic_mpnn_model.ipynb) model, a Chemprop `wMPNN` model is made up of several submodules including a [message passing](./message_passing.ipynb) layer, an [aggregation](./aggregation.ipynb) layer, an optional batch normalization layer, and a [predictor](./predictor.ipynb) feed forward network layer. `wMPNN` defines the training and predicting logic used by `lightning` when using a Chemprop model in their framework.\n",
    "\n",
    "In addition to an `MPNN` model, a `wMPNN` model applies additional atom and bond weightings upon message passing. This is useful in cases where the probability of an atom or bond being present is not always 1. E.g. Polymers, where a 50:50 mixture of two monomers have an atom probability of 0.5:0.5 with the bond probability between monomers being determined by the type of polymer (block, alternating), the type of linking functionalities and the proportion of the monomers.\n",
    "\n",
    "Unlike the `MPNN` model `wMPNN` uses `WeightedBondMessagePassing` as the default message passing scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemprop.nn import WeightedBondMessagePassing, NormAggregation, RegressionFFN\n",
    "\n",
    "mp = WeightedBondMessagePassing()\n",
    "agg = NormAggregation()\n",
    "ffn = RegressionFFN()\n",
    "\n",
    "basic_model = wMPNN(mp, agg, ffn)\n",
    "basic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization can improve training by keeping the inputs to the FFN small and centered around zero. It is off by default, but can be turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wMPNN(mp, agg, ffn, batch_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wMPNN` also configures the optimizer used by lightning during training. The `torch.optim.Adam` optimizer is used with a Noam learning rate scheduler (defined in `chemprop.scheduler.NoamLR`). The following parameters are customizable:\n",
    "\n",
    " - number of warmup epochs, defaults to 2\n",
    " - the initial learning rate, defaults to $10^{-4}$\n",
    " - the max learning rate, defaults to $10^{-3}$\n",
    " - the final learning rate, defaults to $10^{-4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wMPNN(mp, agg, ffn, warmup_epochs=5, init_lr=1e-3, max_lr=1e-2, final_lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the validation and testing loops, lightning will use the metrics stored in `wMPNN` to evaluate the current model's performance. The `wMPNN` has a default metric defined by the type of predictor used. Other [metrics](../metrics.ipynb) can be given to `wMPNN` to use instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemprop.nn import metrics\n",
    "\n",
    "metrics_list = [metrics.RMSE(), metrics.MAE()]\n",
    "model = wMPNN(mp, agg, ffn, metrics=metrics_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fingerprinting and encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wMPNN` has two helper functions to get the hidden representations at different parts of the model. The fingerprint is the learned representation of the message passing layer after aggregation and batch normalization. The encoding is the hidden representation after a number of layers of the predictor. Note that the 0th encoding is equivalent to the fingerprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chemprop.data import PolymerDatapoint, PolymerDataset\n",
    "from chemprop.data import build_dataloader\n",
    "\n",
    "smis = [\n",
    "    \"[*:1]c1cc(F)c([*:2])cc1F.[*:3]c1c(O)cc(O)c([*:4])c1O|0.5|0.5|<1-3:0.5:0.5<1-4:0.5:0.5<2-3:0.5:0.5<2-4:0.5:0.5~10\",\n",
    "    \"[*:1]c1cc(F)c([*:2])cc1F.[*:3]c1c(O)cc(O)c([*:4])c1O|0.5|0.5|<1-2:0.375:0.375<1-1:0.375:0.375<2-2:0.375:0.375<3-4:0.375:0.375<3-3:0.375:0.375<4-4:0.125:0.125<1-3:0.125:0.125<1-4:0.125:0.125<2-3:0.125:0.125<2-4:0.125:0.125\",\n",
    "    ]\n",
    "ys = np.random.rand(len(smis), 1)\n",
    "dataset = PolymerDataset([PolymerDatapoint.from_smi(smi, y) for smi, y in zip(smis, ys)])\n",
    "dataloader = build_dataloader(dataset)\n",
    "batch = next(iter(dataloader))\n",
    "bmg, V_d, X_d, *_ = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model(bmg, V_d, X_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.fingerprint(bmg, V_d, X_d).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.encoding(bmg, V_d, X_d, i=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(basic_model.fingerprint(bmg, V_d, X_d) == basic_model.encoding(bmg, V_d, X_d, i=0)).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
