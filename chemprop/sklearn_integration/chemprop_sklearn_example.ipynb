{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a43c9b",
   "metadata": {},
   "source": [
    "# Chemprop scikit-learn Estimator Example\n",
    "Demonstrating usage of modules in chemprop.sklearn_integration.chemprop_estimator with common scikit-learn workflows including cross-validation and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465acb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemprop.sklearn_integration.chemprop_estimator import ChempropMoleculeTransformer, ChempropReactionTransformer, ChempropMulticomponentTransformer, ChempropRegressor, ChempropEnsembleRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X_mol = np.array([\n",
    "    \"CCO\", \"CCN\", \"CCC\", \"COC\", \"CNC\", \"CCCl\", \"CCBr\", \"CCF\", \"CCI\", \"CC=O\",\n",
    "    \"CC#N\", \"CC(C)O\", \"CC(C)N\", \"CC(C)C\", \"COC(C)\", \"CN(C)C\", \"C1CCCCC1\", \"C1=CC=CC=C1\",\n",
    "    \"CC(C)(C)O\", \"CC(C)(C)N\", \"COCCO\", \"CCOC(=O)C\", \"CCN(CC)CC\", \"CN1CCCC1\", \"C(CO)N\"\n",
    "])\n",
    "X_rxn = np.array([\n",
    "    \"CCO>>CC=O\",\"CCBr.CN>>CCN\",\"CC(=O)O.CCO>>CC(=O)OCC\",\"C=CC=C.C=C>>c1ccccc1\",\"CC(=O)Cl.O>>CC(=O)O\",\n",
    "    \"c1ccccc1.BrBr>>c1ccc(Br)cc1\",\"BrCCBr>>BrC#CBr\",\"ClCCCl.O>>ClCCO\",\"CO.O=C=O>>COC(=O)O\",\"CC(C)(C)Br.CN>>CC(C)(C)CN\",\n",
    "    \"C=O.CC>>CCO\",\"CC=O.CC=O>>CC(O)CC=O\",\"CCBr.C#N>>CC#N\",\"O=CC=O.CC>>O=CC(C)O\",\"CC(=O)OCC.CO>>CC(=O)OCO\",\n",
    "    \"c1ccccc1O.ClCl>>c1ccc(Cl)cc1O\",\"CCBr.O>>CCO\",\"C=C.C=C>>C1=CCC=CC1\",\"CC(=O)O.O>>CC(=O)OO\"\n",
    "])\n",
    "y = np.array([\n",
    "    0.50, 0.60, 0.55, 0.58, 0.52, 0.62, 0.65, 0.57, 0.59, 0.61,\n",
    "    0.56, 0.60, 0.54, 0.53, 0.62, 0.63, 0.45, 0.40,\n",
    "    0.64, 0.66, 0.59, 0.51, 0.48, 0.46, 0.49\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621139f",
   "metadata": {},
   "source": [
    "## Building a pipeline\n",
    "Pipeline ChempropMoleculeTransformer/ChempropReactionTransformer/ChempropMulticomponentTransformer and ChempropRegressor together to obtain an sklearn module that encapulates full chemprop capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f20d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 30.3 K | train\n",
      "1 | agg             | NormAggregation    | 0      | train\n",
      "2 | bn              | Identity           | 0      | train\n",
      "3 | predictor       | RegressionFFN      | 120 K  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "151 K     Trainable params\n",
      "0         Non-trainable params\n",
      "151 K     Total params\n",
      "0.605     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 3/3 [00:00<00:00, 17.22it/s, v_num=205, train_loss_step=0.606, train_loss_epoch=0.991]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 53.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions: [0.5558278 0.5551407 0.5542106 0.5562226 0.555347 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\utilities\\data.py:106: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\utilities\\data.py:106: Total length of `CombinedLoader` across ranks is zero. Please make sure this was your intention.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 252 K  | train\n",
      "1 | agg             | NormAggregation    | 0      | train\n",
      "2 | bn              | Identity           | 0      | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "342 K     Trainable params\n",
      "0         Non-trainable params\n",
      "342 K     Total params\n",
      "1.372     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: No training batches.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\utilities\\data.py:106: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\utilities\\data.py:106: Total length of `CombinedLoader` across ranks is zero. Please make sure this was your intention.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 252 K  | train\n",
      "1 | agg             | NormAggregation    | 0      | train\n",
      "2 | bn              | Identity           | 0      | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "342 K     Trainable params\n",
      "0         Non-trainable params\n",
      "342 K     Total params\n",
      "1.372     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: No training batches.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\utilities\\data.py:106: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\utilities\\data.py:106: Total length of `CombinedLoader` across ranks is zero. Please make sure this was your intention.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 252 K  | train\n",
      "1 | agg             | NormAggregation    | 0      | train\n",
      "2 | bn              | Identity           | 0      | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "342 K     Trainable params\n",
      "0         Non-trainable params\n",
      "342 K     Total params\n",
      "1.372     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: No training batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 60.22it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 62.65it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 59.50it/s]\n",
      "MSE: 0.566638112185843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "mol_pipeline = Pipeline([\n",
    "    (\"featurizer\", ChempropMoleculeTransformer(keep_h=True, add_h=True, ignore_stereo=True, reorder_atoms=True, multi_hot_atom_featurizer_mode='ORGANIC')),\n",
    "    (\"regressor\", ChempropRegressor(batch_size=8, message_hidden_dim=100, depth=5, ffn_num_layers=2, epochs=30, patience=5))\n",
    "])\n",
    "mol_pipeline.fit(X_mol, y)\n",
    "y_pred = mol_pipeline.predict(X_mol[:5])\n",
    "print(f\"Predictions: {y_pred}\")\n",
    "\n",
    "rxn_pipeline = Pipeline([\n",
    "    (\"featurizer\", ChempropReactionTransformer()),\n",
    "    (\"regressor\", ChempropEnsembleRegressor(ensemble_size=3)) #Enables training an ensemble of chemprop regressors and predicting based on their average\n",
    "])\n",
    "rxn_pipeline.fit(X_rxn, y)\n",
    "scores = rxn_pipeline.score(X_rxn[:5],y[:5], metric=\"mse\") # suppot metrics in [\"mae\", \"rmse\", \"mse\", \"r2\", \"accuracy\"] and defaulted to \"rmse\"\n",
    "print(f\"MSE: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c0f78",
   "metadata": {},
   "source": [
    "Alternatively, pass in a path to a csv data file instead of X/y arrays, and include the roles of the columns in arguments of the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2fa92e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[10:15:28] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:01<00:00,  5.86it/s, v_num=209, train_loss_step=0.261, train_loss_epoch=0.300]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:01<00:00,  5.53it/s, v_num=209, train_loss_step=0.261, train_loss_epoch=0.300]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:16:34] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 13.53it/s]\n",
      "RMSE: 1.5933479112215598\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "multicomponent_pipeline = Pipeline([\n",
    "    (\"featurizer\", ChempropMulticomponentTransformer(\n",
    "                    smiles_cols=\"solvent_smiles\",\n",
    "                    rxn_cols=\"rxn_smiles\",\n",
    "                    target_cols=\"target\",\n",
    "                )),\n",
    "    (\"regressor\", ChempropRegressor())\n",
    "])\n",
    "\n",
    "multicomponent_pipeline.fit(X=Path(\"../../tests/data/regression/rxn+mol/rxn+mol.csv\")) #substitute with target datapath\n",
    "score = multicomponent_pipeline.score(X=Path(\"../../tests/data/regression/rxn+mol/rxn+mol.csv\"))\n",
    "print(f\"RMSE: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d5310",
   "metadata": {},
   "source": [
    "## Applying the Sklearn Toolbox\n",
    "You can easily validate the pipeline on your dataset with the sklearn cross_val_score function. However, note that sklearn helpers does not recognize path input, so make sure that the data is inputed as matrix-like X/y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b83cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[10:16:35] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00,  5.70it/s, v_num=210, train_loss_step=0.803, train_loss_epoch=0.837]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00,  5.33it/s, v_num=210, train_loss_step=0.803, train_loss_epoch=0.837]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 22.45it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[10:16:46] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00,  5.74it/s, v_num=211, train_loss_step=0.915, train_loss_epoch=0.879]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00,  5.39it/s, v_num=211, train_loss_step=0.915, train_loss_epoch=0.879]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 23.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[10:16:55] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00,  6.40it/s, v_num=212, train_loss_step=0.634, train_loss_epoch=0.833]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00,  6.08it/s, v_num=212, train_loss_step=0.634, train_loss_epoch=0.833]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 23.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[10:17:05] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00,  6.20it/s, v_num=213, train_loss_step=1.170, train_loss_epoch=0.847]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00,  5.73it/s, v_num=213, train_loss_step=1.170, train_loss_epoch=0.847]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 24.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00,  5.93it/s, v_num=214, train_loss_step=0.662, train_loss_epoch=0.796]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 5/5 [00:00<00:00,  5.55it/s, v_num=214, train_loss_step=0.662, train_loss_epoch=0.796]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:17:24] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 26.97it/s]\n",
      "Cross-validation MSE scores: [ 6.40838618  4.54083211 13.82691184 10.80606021  5.00019431]\n",
      "Average MSE: 8.116476929806362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "\n",
    "multicomponent_pipeline = Pipeline([\n",
    "    # component_types have to be specified for each input column in order for multicomponent transformer, if input is matrix-like\n",
    "    (\"featurizer\", ChempropMulticomponentTransformer(component_types=[\"reaction\", \"molecule\"])),\n",
    "    (\"regressor\", ChempropRegressor(epochs=10))\n",
    "])\n",
    "\n",
    "df = pd.read_csv(\"../../tests/data/regression/rxn+mol/rxn+mol.csv\")\n",
    "X = df[[\"rxn_smiles\",\"solvent_smiles\"]].to_numpy(dtype=str)\n",
    "y = df[\"target\"].to_numpy(dtype=float)\n",
    "\n",
    "scores = cross_val_score(multicomponent_pipeline, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"Cross-validation MSE scores:\", -scores)\n",
    "print(\"Average MSE:\", -scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc8ad9",
   "metadata": {},
   "source": [
    "You can also tune the hyperparameters of the chemprop pipeline with the sklearn GridSearchCV function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e10a2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:38:33] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.89it/s, v_num=313, train_loss_step=1.140, train_loss_epoch=0.888]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.30it/s, v_num=313, train_loss_step=1.140, train_loss_epoch=0.888]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 14.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:38:41] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.88it/s, v_num=314, train_loss_step=1.200, train_loss_epoch=0.886]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.41it/s, v_num=314, train_loss_step=1.200, train_loss_epoch=0.886]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 20.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.67it/s, v_num=315, train_loss_step=1.300, train_loss_epoch=0.805]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.16it/s, v_num=315, train_loss_step=1.300, train_loss_epoch=0.805]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:38:57] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 19.51it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:38:58] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.12it/s, v_num=316, train_loss_step=1.370, train_loss_epoch=0.862]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.49it/s, v_num=316, train_loss_step=1.370, train_loss_epoch=0.862]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 18.66it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:39:06] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.15it/s, v_num=317, train_loss_step=0.594, train_loss_epoch=0.924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.55it/s, v_num=317, train_loss_step=0.594, train_loss_epoch=0.924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.82it/s, v_num=318, train_loss_step=0.995, train_loss_epoch=0.848]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.43it/s, v_num=318, train_loss_step=0.995, train_loss_epoch=0.848]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:39:22] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:39:23] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.09it/s, v_num=319, train_loss_step=0.325, train_loss_epoch=0.858]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.52it/s, v_num=319, train_loss_step=0.325, train_loss_epoch=0.858]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 16.98it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:39:31] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.00it/s, v_num=320, train_loss_step=1.350, train_loss_epoch=0.875]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.46it/s, v_num=320, train_loss_step=1.350, train_loss_epoch=0.875]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 16.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.83it/s, v_num=321, train_loss_step=1.000, train_loss_epoch=0.863]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.31it/s, v_num=321, train_loss_step=1.000, train_loss_epoch=0.863]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:39:47] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:39:47] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.00it/s, v_num=322, train_loss_step=0.668, train_loss_epoch=0.911]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.47it/s, v_num=322, train_loss_step=0.668, train_loss_epoch=0.911]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 16.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:39:55] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.23it/s, v_num=323, train_loss_step=0.905, train_loss_epoch=0.946]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.59it/s, v_num=323, train_loss_step=0.905, train_loss_epoch=0.946]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 19.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.83it/s, v_num=324, train_loss_step=0.849, train_loss_epoch=0.848]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.26it/s, v_num=324, train_loss_step=0.849, train_loss_epoch=0.848]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:40:11] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 19.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:40:12] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.09it/s, v_num=325, train_loss_step=0.780, train_loss_epoch=0.865]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.48it/s, v_num=325, train_loss_step=0.780, train_loss_epoch=0.865]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 16.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:40:20] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.21it/s, v_num=326, train_loss_step=0.322, train_loss_epoch=0.879]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.50it/s, v_num=326, train_loss_step=0.322, train_loss_epoch=0.879]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 15.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.57it/s, v_num=327, train_loss_step=0.254, train_loss_epoch=0.792]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.16it/s, v_num=327, train_loss_step=0.254, train_loss_epoch=0.792]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:40:35] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 16.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:40:36] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.33it/s, v_num=328, train_loss_step=0.939, train_loss_epoch=0.876]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.89it/s, v_num=328, train_loss_step=0.939, train_loss_epoch=0.876]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 18.29it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:40:44] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.04it/s, v_num=329, train_loss_step=0.746, train_loss_epoch=0.898]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.49it/s, v_num=329, train_loss_step=0.746, train_loss_epoch=0.898]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 19.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.87it/s, v_num=330, train_loss_step=1.220, train_loss_epoch=0.867]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.34it/s, v_num=330, train_loss_step=1.220, train_loss_epoch=0.867]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:41:00] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 19.60it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:41:00] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.71it/s, v_num=331, train_loss_step=0.534, train_loss_epoch=0.902]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.27it/s, v_num=331, train_loss_step=0.534, train_loss_epoch=0.902]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 15.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:41:09] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.48it/s, v_num=332, train_loss_step=0.730, train_loss_epoch=0.910]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.02it/s, v_num=332, train_loss_step=0.730, train_loss_epoch=0.910]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 17.56it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.79it/s, v_num=333, train_loss_step=0.972, train_loss_epoch=0.877]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.18it/s, v_num=333, train_loss_step=0.972, train_loss_epoch=0.877]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:41:26] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:41:27] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.49it/s, v_num=334, train_loss_step=1.220, train_loss_epoch=0.903]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  4.95it/s, v_num=334, train_loss_step=1.220, train_loss_epoch=0.903]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 18.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:41:35] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  4.60it/s, v_num=335, train_loss_step=0.950, train_loss_epoch=0.942]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  4.19it/s, v_num=335, train_loss_step=0.950, train_loss_epoch=0.942]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 10.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.73it/s, v_num=336, train_loss_step=1.220, train_loss_epoch=0.875]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.09it/s, v_num=336, train_loss_step=1.220, train_loss_epoch=0.875]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:41:51] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:41:51] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.86it/s, v_num=337, train_loss_step=1.330, train_loss_epoch=0.787]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.17it/s, v_num=337, train_loss_step=1.330, train_loss_epoch=0.787]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 19.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:41:59] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.51it/s, v_num=338, train_loss_step=0.890, train_loss_epoch=0.922]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.14it/s, v_num=338, train_loss_step=0.890, train_loss_epoch=0.922]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 17.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.46it/s, v_num=339, train_loss_step=0.901, train_loss_epoch=0.857]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.03it/s, v_num=339, train_loss_step=0.901, train_loss_epoch=0.857]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:42:16] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 16.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:42:17] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.46it/s, v_num=340, train_loss_step=1.580, train_loss_epoch=0.853]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  4.96it/s, v_num=340, train_loss_step=1.580, train_loss_epoch=0.853]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 13.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:42:26] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.79it/s, v_num=341, train_loss_step=0.845, train_loss_epoch=0.895]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.23it/s, v_num=341, train_loss_step=0.845, train_loss_epoch=0.895]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 15.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.78it/s, v_num=342, train_loss_step=0.593, train_loss_epoch=0.878]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.23it/s, v_num=342, train_loss_step=0.593, train_loss_epoch=0.878]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:42:43] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:42:44] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.42it/s, v_num=343, train_loss_step=1.120, train_loss_epoch=0.856]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  4.89it/s, v_num=343, train_loss_step=1.120, train_loss_epoch=0.856]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 17.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:42:53] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.56it/s, v_num=344, train_loss_step=1.170, train_loss_epoch=0.762]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.05it/s, v_num=344, train_loss_step=1.170, train_loss_epoch=0.762]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 15.87it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.15it/s, v_num=345, train_loss_step=0.677, train_loss_epoch=0.868]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  4.84it/s, v_num=345, train_loss_step=0.677, train_loss_epoch=0.868]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:43:10] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 16.60it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:43:11] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  4.43it/s, v_num=346, train_loss_step=0.531, train_loss_epoch=0.907]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  4.09it/s, v_num=346, train_loss_step=0.531, train_loss_epoch=0.907]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 19.88it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:43:20] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.53it/s, v_num=347, train_loss_step=0.922, train_loss_epoch=0.909]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.16it/s, v_num=347, train_loss_step=0.922, train_loss_epoch=0.909]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.58it/s, v_num=348, train_loss_step=1.090, train_loss_epoch=0.869]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.15it/s, v_num=348, train_loss_step=1.090, train_loss_epoch=0.869]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:43:36] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:43:37] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.45it/s, v_num=349, train_loss_step=1.260, train_loss_epoch=0.911]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.08it/s, v_num=349, train_loss_step=1.260, train_loss_epoch=0.911]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 14.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:43:46] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.49it/s, v_num=350, train_loss_step=1.290, train_loss_epoch=0.904]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.82it/s, v_num=350, train_loss_step=1.290, train_loss_epoch=0.904]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 17.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.59it/s, v_num=351, train_loss_step=0.769, train_loss_epoch=0.797]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.06it/s, v_num=351, train_loss_step=0.769, train_loss_epoch=0.797]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:44:02] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 14.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:44:03] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:01<00:00,  3.88it/s, v_num=352, train_loss_step=0.527, train_loss_epoch=0.887]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:01<00:00,  3.62it/s, v_num=352, train_loss_step=0.527, train_loss_epoch=0.887]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:44:13] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:01<00:00,  3.52it/s, v_num=353, train_loss_step=0.694, train_loss_epoch=0.953]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:01<00:00,  3.23it/s, v_num=353, train_loss_step=0.694, train_loss_epoch=0.953]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00,  9.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.25it/s, v_num=354, train_loss_step=1.410, train_loss_epoch=0.828]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.89it/s, v_num=354, train_loss_step=1.410, train_loss_epoch=0.828]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:44:35] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 19.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:44:36] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:01<00:00,  3.47it/s, v_num=355, train_loss_step=0.827, train_loss_epoch=0.916]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:01<00:00,  3.28it/s, v_num=355, train_loss_step=0.827, train_loss_epoch=0.916]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 11.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:44:46] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  4.18it/s, v_num=356, train_loss_step=0.687, train_loss_epoch=0.897]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:01<00:00,  3.90it/s, v_num=356, train_loss_step=0.687, train_loss_epoch=0.897]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 14.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  4.31it/s, v_num=357, train_loss_step=1.080, train_loss_epoch=0.866]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:01<00:00,  3.85it/s, v_num=357, train_loss_step=1.080, train_loss_epoch=0.866]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:45:11] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 13.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:45:11] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  7.84it/s, v_num=358, train_loss_step=0.836, train_loss_epoch=0.907]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  7.16it/s, v_num=358, train_loss_step=0.836, train_loss_epoch=0.907]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 27.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:45:20] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.98it/s, v_num=359, train_loss_step=0.622, train_loss_epoch=0.812]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.99it/s, v_num=359, train_loss_step=0.622, train_loss_epoch=0.812]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 25.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.68it/s, v_num=360, train_loss_step=0.540, train_loss_epoch=0.840]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  7.90it/s, v_num=360, train_loss_step=0.540, train_loss_epoch=0.840]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:45:29] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 29.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:45:30] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.84it/s, v_num=361, train_loss_step=0.601, train_loss_epoch=0.893]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.91it/s, v_num=361, train_loss_step=0.601, train_loss_epoch=0.893]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 28.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:45:35] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.51it/s, v_num=362, train_loss_step=1.110, train_loss_epoch=0.949]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.78it/s, v_num=362, train_loss_step=1.110, train_loss_epoch=0.949]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 27.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.25it/s, v_num=363, train_loss_step=1.030, train_loss_epoch=0.821]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.58it/s, v_num=363, train_loss_step=1.030, train_loss_epoch=0.821]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:45:45] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 30.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:45:45] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.41it/s, v_num=364, train_loss_step=0.726, train_loss_epoch=0.909]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.71it/s, v_num=364, train_loss_step=0.726, train_loss_epoch=0.909]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 27.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:45:50] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 10.07it/s, v_num=365, train_loss_step=1.000, train_loss_epoch=0.912]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.09it/s, v_num=365, train_loss_step=1.000, train_loss_epoch=0.912]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 28.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.89it/s, v_num=366, train_loss_step=0.898, train_loss_epoch=0.891]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.23it/s, v_num=366, train_loss_step=0.898, train_loss_epoch=0.891]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:46:00] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 31.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:46:00] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.90it/s, v_num=367, train_loss_step=0.693, train_loss_epoch=0.875]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.94it/s, v_num=367, train_loss_step=0.693, train_loss_epoch=0.875]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 27.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:46:05] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.68it/s, v_num=368, train_loss_step=1.190, train_loss_epoch=0.906]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.76it/s, v_num=368, train_loss_step=1.190, train_loss_epoch=0.906]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 27.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.18it/s, v_num=369, train_loss_step=0.738, train_loss_epoch=0.852]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.28it/s, v_num=369, train_loss_step=0.738, train_loss_epoch=0.852]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:46:15] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 32.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:46:15] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.48it/s, v_num=370, train_loss_step=0.884, train_loss_epoch=0.897]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.37it/s, v_num=370, train_loss_step=0.884, train_loss_epoch=0.897]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 25.81it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:46:21] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.97it/s, v_num=371, train_loss_step=0.603, train_loss_epoch=0.948]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.95it/s, v_num=371, train_loss_step=0.603, train_loss_epoch=0.948]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 22.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.82it/s, v_num=372, train_loss_step=1.600, train_loss_epoch=0.871]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  7.95it/s, v_num=372, train_loss_step=1.600, train_loss_epoch=0.871]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:46:31] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 33.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:46:31] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.52it/s, v_num=373, train_loss_step=0.972, train_loss_epoch=0.885]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.65it/s, v_num=373, train_loss_step=0.972, train_loss_epoch=0.885]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 26.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:46:36] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.60it/s, v_num=374, train_loss_step=0.788, train_loss_epoch=0.900]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.88it/s, v_num=374, train_loss_step=0.788, train_loss_epoch=0.900]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 28.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.28it/s, v_num=375, train_loss_step=1.040, train_loss_epoch=0.873]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.64it/s, v_num=375, train_loss_step=1.040, train_loss_epoch=0.873]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:46:46] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 30.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:46:46] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 10.21it/s, v_num=376, train_loss_step=0.573, train_loss_epoch=0.935]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.19it/s, v_num=376, train_loss_step=0.573, train_loss_epoch=0.935]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 29.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:46:52] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.69it/s, v_num=377, train_loss_step=0.658, train_loss_epoch=0.919]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.76it/s, v_num=377, train_loss_step=0.658, train_loss_epoch=0.919]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 31.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.69it/s, v_num=378, train_loss_step=0.855, train_loss_epoch=0.845]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.09it/s, v_num=378, train_loss_step=0.855, train_loss_epoch=0.845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:47:02] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 32.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:47:02] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.85it/s, v_num=379, train_loss_step=0.887, train_loss_epoch=0.898]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.91it/s, v_num=379, train_loss_step=0.887, train_loss_epoch=0.898]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 29.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:47:07] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 10.23it/s, v_num=380, train_loss_step=0.726, train_loss_epoch=0.888]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.28it/s, v_num=380, train_loss_step=0.726, train_loss_epoch=0.888]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 30.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.41it/s, v_num=381, train_loss_step=1.340, train_loss_epoch=0.865]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.57it/s, v_num=381, train_loss_step=1.340, train_loss_epoch=0.865]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:47:17] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 33.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:47:17] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 10.51it/s, v_num=382, train_loss_step=1.060, train_loss_epoch=0.899]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.50it/s, v_num=382, train_loss_step=1.060, train_loss_epoch=0.899]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:47:22] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.97it/s, v_num=383, train_loss_step=1.000, train_loss_epoch=0.910]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.07it/s, v_num=383, train_loss_step=1.000, train_loss_epoch=0.910]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 22.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.47it/s, v_num=384, train_loss_step=0.640, train_loss_epoch=0.897]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.55it/s, v_num=384, train_loss_step=0.640, train_loss_epoch=0.897]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:47:32] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 33.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:47:32] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.45it/s, v_num=385, train_loss_step=0.813, train_loss_epoch=0.891]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.74it/s, v_num=385, train_loss_step=0.813, train_loss_epoch=0.891]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 27.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:47:37] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 10.00it/s, v_num=386, train_loss_step=0.672, train_loss_epoch=0.901]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.02it/s, v_num=386, train_loss_step=0.672, train_loss_epoch=0.901]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 30.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.81it/s, v_num=387, train_loss_step=0.795, train_loss_epoch=0.868]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.88it/s, v_num=387, train_loss_step=0.795, train_loss_epoch=0.868]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 30.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.75it/s, v_num=388, train_loss_step=0.416, train_loss_epoch=0.892]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.84it/s, v_num=388, train_loss_step=0.416, train_loss_epoch=0.892]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 28.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:47:52] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.69it/s, v_num=389, train_loss_step=1.160, train_loss_epoch=0.909]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.82it/s, v_num=389, train_loss_step=1.160, train_loss_epoch=0.909]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 30.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.27it/s, v_num=390, train_loss_step=1.260, train_loss_epoch=0.893]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.47it/s, v_num=390, train_loss_step=1.260, train_loss_epoch=0.893]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:48:02] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 28.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:48:02] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  7.06it/s, v_num=391, train_loss_step=0.966, train_loss_epoch=0.879]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.58it/s, v_num=391, train_loss_step=0.966, train_loss_epoch=0.879]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 28.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:48:07] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.60it/s, v_num=392, train_loss_step=0.704, train_loss_epoch=0.807]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.64it/s, v_num=392, train_loss_step=0.704, train_loss_epoch=0.807]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 22.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.95it/s, v_num=393, train_loss_step=1.100, train_loss_epoch=0.843]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.32it/s, v_num=393, train_loss_step=1.100, train_loss_epoch=0.843]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:48:18] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 21.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:48:18] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.69it/s, v_num=394, train_loss_step=0.639, train_loss_epoch=0.826]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.78it/s, v_num=394, train_loss_step=0.639, train_loss_epoch=0.826]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 30.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:48:23] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.27it/s, v_num=395, train_loss_step=0.526, train_loss_epoch=0.924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.56it/s, v_num=395, train_loss_step=0.526, train_loss_epoch=0.924]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 29.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.85it/s, v_num=396, train_loss_step=1.050, train_loss_epoch=0.860]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.24it/s, v_num=396, train_loss_step=1.050, train_loss_epoch=0.860]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:48:33] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 30.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:48:33] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.22it/s, v_num=397, train_loss_step=0.731, train_loss_epoch=0.874]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.38it/s, v_num=397, train_loss_step=0.731, train_loss_epoch=0.874]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 31.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:48:38] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.71it/s, v_num=398, train_loss_step=0.615, train_loss_epoch=0.962]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.77it/s, v_num=398, train_loss_step=0.615, train_loss_epoch=0.962]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 29.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.13it/s, v_num=399, train_loss_step=0.977, train_loss_epoch=0.849]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  7.51it/s, v_num=399, train_loss_step=0.977, train_loss_epoch=0.849]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:48:48] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 35.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:48:48] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.80it/s, v_num=400, train_loss_step=0.412, train_loss_epoch=0.908]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.86it/s, v_num=400, train_loss_step=0.412, train_loss_epoch=0.908]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 29.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:48:53] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.61it/s, v_num=401, train_loss_step=0.802, train_loss_epoch=0.957]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.70it/s, v_num=401, train_loss_step=0.802, train_loss_epoch=0.957]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 28.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.17it/s, v_num=402, train_loss_step=0.866, train_loss_epoch=0.858]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.40it/s, v_num=402, train_loss_step=0.866, train_loss_epoch=0.858]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:49:03] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 34.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:49:03] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.71it/s, v_num=403, train_loss_step=0.899, train_loss_epoch=0.892]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.97it/s, v_num=403, train_loss_step=0.899, train_loss_epoch=0.892]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 28.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:49:08] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.63it/s, v_num=404, train_loss_step=0.851, train_loss_epoch=0.941]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.05it/s, v_num=404, train_loss_step=0.851, train_loss_epoch=0.941]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  9.11it/s, v_num=405, train_loss_step=0.825, train_loss_epoch=0.847]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  8.29it/s, v_num=405, train_loss_step=0.825, train_loss_epoch=0.847]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:49:18] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 35.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:49:18] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.97it/s, v_num=406, train_loss_step=1.090, train_loss_epoch=0.889]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  6.40it/s, v_num=406, train_loss_step=1.090, train_loss_epoch=0.889]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 16.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:49:26] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  7.91it/s, v_num=407, train_loss_step=0.686, train_loss_epoch=0.924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  7.18it/s, v_num=407, train_loss_step=0.686, train_loss_epoch=0.924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 17.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  5.01it/s, v_num=408, train_loss_step=1.400, train_loss_epoch=0.857]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00,  4.65it/s, v_num=408, train_loss_step=1.400, train_loss_epoch=0.857]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:49:40] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 14.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[11:49:40] WARNING: not removing hydrogen atom without neighbors\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\jxl05\\miniconda3\\envs\\chemprop1\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 480 K  | train\n",
      "1 | agg             | NormAggregation              | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "660 K     Trainable params\n",
      "0         Non-trainable params\n",
      "660 K     Total params\n",
      "2.642     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 6/6 [00:01<00:00,  4.91it/s, v_num=409, train_loss_step=0.703, train_loss_epoch=0.877]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 6/6 [00:01<00:00,  4.64it/s, v_num=409, train_loss_step=0.703, train_loss_epoch=0.877]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;featurizer&#x27;,\n",
       "                                        ChempropMulticomponentTransformer(component_types=[&#x27;reaction&#x27;,\n",
       "                                                                                           &#x27;molecule&#x27;])),\n",
       "                                       (&#x27;regressor&#x27;,\n",
       "                                        ChempropRegressor(epochs=10,\n",
       "                                                          patience=10))]),\n",
       "             param_grid={&#x27;regressor__depth&#x27;: [3, 6],\n",
       "                         &#x27;regressor__dropout&#x27;: [0, 0.2],\n",
       "                         &#x27;regressor__ffn_hidden_dim&#x27;: [300, 1000, 1700, 2400],\n",
       "                         &#x27;regressor__ffn_num_layers&#x27;: [1, 2]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('estimator',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">estimator&nbsp;</td>\n",
       "            <td class=\"value\">Pipeline(step...atience=10))])</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('param_grid',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">param_grid&nbsp;</td>\n",
       "            <td class=\"value\">{&#x27;regressor__depth&#x27;: [3, 6], &#x27;regressor__dropout&#x27;: [0, 0.2], &#x27;regressor__ffn_hidden_dim&#x27;: [300, 1000, ...], &#x27;regressor__ffn_num_layers&#x27;: [1, 2]}</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('scoring',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">scoring&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;neg_mean_squared_error&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('refit',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">refit&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cv',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">cv&nbsp;</td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('pre_dispatch',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">pre_dispatch&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;2*n_jobs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('error_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">error_score&nbsp;</td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('return_train_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">return_train_score&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: Pipeline</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___\"></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>ChempropMulticomponentTransformer</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___featurizer__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('component_types',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">component_types&nbsp;</td>\n",
       "            <td class=\"value\">[&#x27;reaction&#x27;, &#x27;molecule&#x27;]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('reaction_mode',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">reaction_mode&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;REAC_DIFF&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_hot_atom_featurizer_mode',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_hot_atom_featurizer_mode&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;V2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('keep_h',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">keep_h&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('add_h',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">add_h&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ignore_stereo',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ignore_stereo&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('reorder_atoms',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">reorder_atoms&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('smiles_cols',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">smiles_cols&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('rxn_cols',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">rxn_cols&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('target_cols',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">target_cols&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ignore_cols',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ignore_cols&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('weight_col',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">weight_col&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('bounded',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">bounded&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('no_header_row',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">no_header_row&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>ChempropRegressor</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___regressor__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('num_workers',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">num_workers&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('batch_size',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">batch_size&nbsp;</td>\n",
       "            <td class=\"value\">64</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('output_dir',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">output_dir&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('checkpoint',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">checkpoint&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('molecule_featurizers',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">molecule_featurizers&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('no_descriptor_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">no_descriptor_scaling&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('message_hidden_dim',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">message_hidden_dim&nbsp;</td>\n",
       "            <td class=\"value\">300</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('depth',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">depth&nbsp;</td>\n",
       "            <td class=\"value\">6</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dropout',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dropout&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('aggregation',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">aggregation&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;norm&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ffn_hidden_dim',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ffn_hidden_dim&nbsp;</td>\n",
       "            <td class=\"value\">1700</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ffn_num_layers',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ffn_num_layers&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('batch_norm',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">batch_norm&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multiclass_num_classes',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multiclass_num_classes&nbsp;</td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('accelerator',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">accelerator&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('devices',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">devices&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('epochs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">epochs&nbsp;</td>\n",
       "            <td class=\"value\">10</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('patience',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">patience&nbsp;</td>\n",
       "            <td class=\"value\">10</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('no_cache',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">no_cache&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('task_type',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">task_type&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;regression&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('loss_function',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">loss_function&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('metrics',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">metrics&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('featurizer',\n",
       "                                        ChempropMulticomponentTransformer(component_types=['reaction',\n",
       "                                                                                           'molecule'])),\n",
       "                                       ('regressor',\n",
       "                                        ChempropRegressor(epochs=10,\n",
       "                                                          patience=10))]),\n",
       "             param_grid={'regressor__depth': [3, 6],\n",
       "                         'regressor__dropout': [0, 0.2],\n",
       "                         'regressor__ffn_hidden_dim': [300, 1000, 1700, 2400],\n",
       "                         'regressor__ffn_num_layers': [1, 2]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'regressor__dropout': [0,0.2],\n",
    "    'regressor__depth': [3, 6],\n",
    "    'regressor__ffn_hidden_dim': [300, 1000, 1700, 2400],\n",
    "    'regressor__ffn_num_layers': [1,2]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(multicomponent_pipeline, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b17970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'regressor__depth': 6, 'regressor__dropout': 0, 'regressor__ffn_hidden_dim': 1700, 'regressor__ffn_num_layers': 1}\n",
      "Best score (MSE): 8.116683249601653\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best score (MSE):\", -grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac5479",
   "metadata": {},
   "source": [
    "## Saving model(s) & Reloading from checkpoint\n",
    "Save a model as best.pt with the _save_model method of the regressor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ebb358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ChempropRegressor(dropout=0.2, epochs=10, ffn_hidden_dim=2400, ffn_num_layers=2,\n",
       "                  patience=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>ChempropRegressor</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('num_workers',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">num_workers&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('batch_size',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">batch_size&nbsp;</td>\n",
       "            <td class=\"value\">64</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('output_dir',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">output_dir&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('checkpoint',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">checkpoint&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('molecule_featurizers',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">molecule_featurizers&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('no_descriptor_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">no_descriptor_scaling&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('message_hidden_dim',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">message_hidden_dim&nbsp;</td>\n",
       "            <td class=\"value\">300</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('depth',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">depth&nbsp;</td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dropout',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dropout&nbsp;</td>\n",
       "            <td class=\"value\">0.2</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('aggregation',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">aggregation&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;norm&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ffn_hidden_dim',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ffn_hidden_dim&nbsp;</td>\n",
       "            <td class=\"value\">2400</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ffn_num_layers',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ffn_num_layers&nbsp;</td>\n",
       "            <td class=\"value\">2</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('batch_norm',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">batch_norm&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multiclass_num_classes',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multiclass_num_classes&nbsp;</td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('accelerator',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">accelerator&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('devices',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">devices&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('epochs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">epochs&nbsp;</td>\n",
       "            <td class=\"value\">10</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('patience',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">patience&nbsp;</td>\n",
       "            <td class=\"value\">10</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('no_cache',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">no_cache&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('task_type',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">task_type&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;regression&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('loss_function',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">loss_function&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('metrics',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">metrics&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "ChempropRegressor(dropout=0.2, epochs=10, ffn_hidden_dim=2400, ffn_num_layers=2,\n",
       "                  patience=10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_regressor = grid.best_estimator_[\"regressor\"]\n",
    "best_regressor._save_model(\"checkpoints\") # make sure to point to an existing directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d348681",
   "metadata": {},
   "source": [
    "Load a model by specifying the checkpoint argument of the regressor with a list of paths to .pt/.ckpt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28144fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model_path = Path(\"../../tests/data/example_model_v2_regression_rxn+mol.pt\")\n",
    "loaded_regressor = ChempropRegressor(checkpoint = [model_path])\n",
    "\n",
    "#load multiple models with the ensemble regressor\n",
    "loaded_ensemble_regressor = ChempropRegressor(checkpoint = [model_path, model_path])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
